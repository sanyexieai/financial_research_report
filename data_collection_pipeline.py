#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ”¶é›†æµç¨‹
è´Ÿè´£æ”¶é›†è´¢åŠ¡æ•°æ®ã€å…¬å¸ä¿¡æ¯ã€è¡Œä¸šä¿¡æ¯ç­‰ï¼Œå¹¶å°†æ•°æ®å‘é‡åŒ–å­˜å‚¨åˆ°PostgreSQLæ•°æ®åº“
"""

import os
import glob
import time
import json
import logging
from datetime import datetime
from dotenv import load_dotenv

from data_analysis_agent.config.llm_config import LLMConfig
from data_analysis_agent.utils.llm_helper import LLMHelper
from utils.get_shareholder_info import get_shareholder_info, get_table_content
from utils.get_financial_statements import get_all_financial_statements, save_financial_statements_to_csv
from utils.identify_competitors import identify_competitors_with_ai
from utils.get_stock_intro import get_stock_intro, save_stock_intro_to_txt
from utils.search_engine import SearchEngine
from utils.rag_postgres import RAGPostgresHelper
from config.database_config import db_config

class DataCollectionPipeline:
    """æ•°æ®æ”¶é›†æµç¨‹ç±»"""
    
    def __init__(self, target_company="å•†æ±¤ç§‘æŠ€", target_company_code="00020", target_company_market="HK", search_engine="all"):
        # é…ç½®æ—¥å¿—è®°å½•
        self.setup_logging()
        
        # ç¯å¢ƒå˜é‡ä¸å…¨å±€é…ç½®
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4")
        
        self.logger.info(f"ğŸ”§ ä½¿ç”¨çš„æ¨¡å‹: {self.model}")
        self.target_company = target_company
        self.target_company_code = target_company_code
        self.target_company_market = target_company_market
        
        # æœç´¢å¼•æ“é…ç½®
        self.search_engine = SearchEngine()
        if search_engine and search_engine != "all":
            self.logger.info(f"ğŸ” æœç´¢å¼•æ“å·²é…ç½®ä¸º: {search_engine.upper()}")
        else:
            self.logger.info(f"ğŸ” æœç´¢å¼•æ“é»˜è®¤å…¨éƒ¨ä½¿ç”¨")
        
        # ç›®å½•é…ç½®
        self.data_dir = "./download_financial_statement_files"
        self.company_info_dir = "./company_info"
        self.industry_info_dir = "./industry_info"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for dir_path in [self.data_dir, self.company_info_dir, self.industry_info_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # LLMé…ç½®
        self.llm_config = LLMConfig(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model,
            temperature=0.7,
            max_tokens=8192,
        )
        self.llm = LLMHelper(self.llm_config)
        
        # åˆå§‹åŒ–PostgreSQL RAGåŠ©æ‰‹
        try:
            self.logger.info("ğŸ”— åˆå§‹åŒ–PostgreSQL RAGåŠ©æ‰‹...")
            self.rag_helper = RAGPostgresHelper(
                db_config=db_config.get_postgres_config(),
                rag_config=db_config.get_rag_config()
            )
            self.logger.info("âœ… PostgreSQL RAGåŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"âŒ PostgreSQL RAGåŠ©æ‰‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def setup_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•"""
        # åˆ›å»ºlogsç›®å½•
        os.makedirs("logs", exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"logs/data_collection_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger('DataCollection')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        self.logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"ğŸ“ æ—¥å¿—è®°å½•å·²å¯åŠ¨ï¼Œæ—¥å¿—æ–‡ä»¶: {log_filename}")
    
    def collect_competitors(self):
        """æ”¶é›†ç«äº‰å¯¹æ‰‹ä¿¡æ¯"""
        self.logger.info("ğŸ” è¯†åˆ«ç«äº‰å¯¹æ‰‹...")
        other_companies = identify_competitors_with_ai(
            api_key=self.api_key,
            base_url=self.base_url,
            model_name=self.model,
            company_name=self.target_company
        )
        listed_companies = [company for company in other_companies if company.get('market') != "æœªä¸Šå¸‚"]
        
        # å°†ç«äº‰å¯¹æ‰‹ä¿¡æ¯å­˜å‚¨åˆ°æ•°æ®åº“
        competitors_text = f"ç›®æ ‡å…¬å¸: {self.target_company}\nç«äº‰å¯¹æ‰‹åˆ—è¡¨:\n"
        for company in listed_companies:
            competitors_text += f"- {company.get('name', '')} ({company.get('code', '')}) - {company.get('market', '')}\n"
        
        # æ·»åŠ åˆ°RAGæ•°æ®åº“
        self.rag_helper.add_search_results([{
            'title': f'{self.target_company}ç«äº‰å¯¹æ‰‹åˆ†æ',
            'description': competitors_text,
            'url': 'internal://competitors'
        }], f"{self.target_company}ç«äº‰å¯¹æ‰‹")
        
        self.logger.info(f"âœ… è¯†åˆ«åˆ° {len(listed_companies)} ä¸ªç«äº‰å¯¹æ‰‹")
        return listed_companies
    
    def collect_financial_data(self, listed_companies):
        """æ”¶é›†è´¢åŠ¡æ•°æ®"""
        all_companies = [(self.target_company, self.target_company_code, self.target_company_market)]
        
        # æ·»åŠ ç«äº‰å¯¹æ‰‹
        for company in listed_companies:
            company_name = company.get('name')
            company_code = company.get('code')
            market_str = company.get('market', '')
            
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            
            all_companies.append((company_name, company_code, market))
        
        # æ”¶é›†æ‰€æœ‰å…¬å¸çš„è´¢åŠ¡æ•°æ®
        for company_name, company_code, market in all_companies:
            self.logger.info(f"ğŸ“Š è·å– {company_name}({market}:{company_code}) çš„è´¢åŠ¡æ•°æ®...")
            try:
                financials = get_all_financial_statements(
                    stock_code=company_code,
                    market=market,
                    period="å¹´åº¦",
                    verbose=False
                )
                
                # ä¿å­˜åˆ°CSVæ–‡ä»¶
                save_financial_statements_to_csv(
                    financial_statements=financials,
                    stock_code=company_code,
                    market=market,
                    company_name=company_name,
                    period="å¹´åº¦",
                    save_dir=self.data_dir
                )
                
                # å°†è´¢åŠ¡æ•°æ®æ‘˜è¦å­˜å‚¨åˆ°æ•°æ®åº“
                if financials:
                    financial_summary = f"{company_name}è´¢åŠ¡æ•°æ®æ‘˜è¦:\n"
                    for statement_type, data in financials.items():
                        if data and len(data) > 0:
                            financial_summary += f"{statement_type}: {len(data)}æ¡è®°å½•\n"
                    
                    self.rag_helper.add_search_results([{
                        'title': f'{company_name}è´¢åŠ¡æ•°æ®',
                        'description': financial_summary,
                        'url': f'internal://financial/{company_code}'
                    }], f"{company_name}è´¢åŠ¡æ•°æ®")
                
                self.logger.info(f"  âœ… {company_name} è´¢åŠ¡æ•°æ®æ”¶é›†å®Œæˆ")
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f"  âŒ è·å– {company_name} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
    
    def collect_company_info(self, listed_companies):
        """æ”¶é›†å…¬å¸åŸºç¡€ä¿¡æ¯"""
        all_companies = [(self.target_company, self.target_company_code, self.target_company_market)]
        
        # æ·»åŠ ç«äº‰å¯¹æ‰‹
        for company in listed_companies:
            company_name = company.get('name')
            company_code = company.get('code')
            market_str = company.get('market', '')
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            all_companies.append((company_name, company_code, market))
        
        # æ·»åŠ ç‰¹å®šå…¬å¸å¦‚ç™¾åº¦
        all_companies.append(("ç™¾åº¦", "09888", "HK"))
        
        for company_name, company_code, market in all_companies:
            self.logger.info(f"ğŸ¢ è·å– {company_name}({market}:{company_code}) çš„åŸºç¡€ä¿¡æ¯...")
            try:
                company_info = get_stock_intro(company_code, market=market)
                if company_info:
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    save_path = os.path.join(self.company_info_dir, f"{company_name}_{market}_{company_code}_info.txt")
                    save_stock_intro_to_txt(company_code, market, save_path)
                    
                    # å­˜å‚¨åˆ°æ•°æ®åº“
                    self.rag_helper.add_search_results([{
                        'title': f'{company_name}å…¬å¸ä»‹ç»',
                        'description': company_info,
                        'url': f'internal://company/{company_code}'
                    }], f"{company_name}å…¬å¸ä¿¡æ¯")
                    
                    self.logger.info(f"  âœ… {company_name} åŸºç¡€ä¿¡æ¯æ”¶é›†å®Œæˆ")
                else:
                    self.logger.warning(f"  âš ï¸ æœªèƒ½è·å–åˆ° {company_name} çš„åŸºç¡€ä¿¡æ¯")
                time.sleep(1)
                
            except Exception as e:
                self.logger.error(f"  âŒ è·å– {company_name} åŸºç¡€ä¿¡æ¯å¤±è´¥: {e}")
    
    def collect_shareholder_info(self):
        """æ”¶é›†è‚¡ä¸œä¿¡æ¯"""
        self.logger.info("ğŸ‘¥ è·å–è‚¡ä¸œä¿¡æ¯...")
        try:
            info = get_shareholder_info()
            shangtang_shareholder_info = info.get("tables")
            table_content = get_table_content(shangtang_shareholder_info)
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            self.rag_helper.add_search_results([{
                'title': f'{self.target_company}è‚¡ä¸œç»“æ„',
                'description': table_content,
                'url': 'internal://shareholder'
            }], f"{self.target_company}è‚¡ä¸œä¿¡æ¯")
            
            self.logger.info("âœ… è‚¡ä¸œä¿¡æ¯æ”¶é›†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–è‚¡ä¸œä¿¡æ¯å¤±è´¥: {e}")
    
    def collect_industry_info(self, listed_companies):
        """æ”¶é›†è¡Œä¸šä¿¡æ¯"""
        self.logger.info("ğŸ” æœç´¢è¡Œä¸šä¿¡æ¯...")
        all_companies = [self.target_company] + [company.get('name') for company in listed_companies]
        
        for company_name in all_companies:
            search_keywords = f"{company_name} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ç«äº‰åˆ†æ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥"
            self.logger.info(f"  æ­£åœ¨æœç´¢: {search_keywords}")
            
            try:
                results = self.search_engine.search(search_keywords, 10)
                
                # å­˜å‚¨åˆ°æ•°æ®åº“
                for result in results:
                    self.rag_helper.add_search_results([result], f"{company_name}è¡Œä¸šä¿¡æ¯")
                
                self.logger.info(f"  âœ… {company_name} è¡Œä¸šä¿¡æ¯æ”¶é›†å®Œæˆï¼Œå…± {len(results)} æ¡ç»“æœ")
                
                # å¢åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(self.search_engine.delay * 2)
                
            except Exception as e:
                self.logger.error(f"  âŒ æœç´¢ {company_name} è¡Œä¸šä¿¡æ¯å¤±è´¥: {e}")
        
        # ä¿å­˜æœç´¢ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¤‡ä»½ï¼‰
        search_results_file = os.path.join(self.industry_info_dir, "all_search_results.json")
        with open(search_results_file, 'w', encoding='utf-8') as f:
            json.dump({company: [] for company in all_companies}, f, ensure_ascii=False, indent=2)
    
    def run_data_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸš€ å¼€å§‹æ•°æ®æ”¶é›†æµç¨‹")
        self.logger.info("="*80)
        
        try:
            # 1. æ”¶é›†ç«äº‰å¯¹æ‰‹ä¿¡æ¯
            listed_companies = self.collect_competitors()
            
            # 2. æ”¶é›†è´¢åŠ¡æ•°æ®
            self.collect_financial_data(listed_companies)
            
            # 3. æ”¶é›†å…¬å¸åŸºç¡€ä¿¡æ¯
            self.collect_company_info(listed_companies)
            
            # 4. æ”¶é›†è‚¡ä¸œä¿¡æ¯
            self.collect_shareholder_info()
            
            # 5. æ”¶é›†è¡Œä¸šä¿¡æ¯
            self.collect_industry_info(listed_companies)
            
            # 6. æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡
            stats = self.rag_helper.get_statistics()
            self.logger.info(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
            self.logger.info(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
            self.logger.info(f"  æ€»æ–‡æ¡£å—: {stats['total_chunks']}")
            self.logger.info(f"  æœ€æ–°æ›´æ–°: {stats.get('last_updated', 'æœªçŸ¥')}")
            
            self.logger.info("\nâœ… æ•°æ®æ”¶é›†æµç¨‹å®Œæˆï¼")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æ”¶é›†æµç¨‹å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ•°æ®æ”¶é›†æµç¨‹')
    parser.add_argument('--company', default='å•†æ±¤ç§‘æŠ€', help='ç›®æ ‡å…¬å¸åç§°')
    parser.add_argument('--code', default='00020', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--market', default='HK', help='å¸‚åœºä»£ç ')
    parser.add_argument('--search-engine', choices=['ddg', 'sogou', 'all'], default='all',
                       help='æœç´¢å¼•æ“é€‰æ‹©')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ•°æ®æ”¶é›†å®ä¾‹
    pipeline = DataCollectionPipeline(
        target_company=args.company,
        target_company_code=args.code,
        target_company_market=args.market,
        search_engine=args.search_engine
    )
    
    # è¿è¡Œæ•°æ®æ”¶é›†æµç¨‹
    success = pipeline.run_data_collection()
    
    if success:
        print("\nğŸ‰ æ•°æ®æ”¶é›†æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
        print("ğŸ“Š æ‰€æœ‰æ•°æ®å·²å‘é‡åŒ–å­˜å‚¨åˆ°PostgreSQLæ•°æ®åº“")
    else:
        print("\nâŒ æ•°æ®æ”¶é›†æµç¨‹æ‰§è¡Œå¤±è´¥ï¼")


if __name__ == "__main__":
    main() 