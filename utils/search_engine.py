"""
æœç´¢å¼•æ“å°è£…æ¨¡å—
æ”¯æŒ DuckDuckGo å’Œ Sogou ä¸¤ç§æœç´¢æ–¹å¼
"""

import time
import logging
import json
import os
from typing import List, Dict, Any
from duckduckgo_search import DDGS
from utils.googlenews_utils import GoogleNewsSearch
from datetime import datetime, timedelta

# å°è¯•å¯¼å…¥æœç‹—æœç´¢ï¼Œå¦‚æœå¤±è´¥åˆ™åªæ”¯æŒDDG
try:
    from sogou_search import sogou_search
    SOGOU_AVAILABLE = True
except ImportError:
    SOGOU_AVAILABLE = False

class SearchEngine:
    """æœç´¢å¼•æ“å°è£…ç±»ï¼Œæ”¯æŒå¤šå¼•æ“åˆå¹¶å»é‡"""

    def __init__(self, engine=None, cache_dir="search_cache", cache_expire_days=3):
        """
        åˆå§‹åŒ–æœç´¢å¼•æ“

        Args:
            engine: æœç´¢å¼•æ“ç±»å‹ï¼Œæ”¯æŒ "ddg" (DuckDuckGo)ã€"sogou" (æœç‹—)ã€"google" (Google News) æˆ–å®ƒä»¬çš„åˆ—è¡¨ã€‚è‹¥ä¸ºç©ºåˆ™é»˜è®¤å…¨éƒ¨ã€‚
            cache_dir: ç¼“å­˜ç›®å½•
            cache_expire_days: ç¼“å­˜è¿‡æœŸå¤©æ•°ï¼Œé»˜è®¤3å¤©
        """
        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self.setup_logging()
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = cache_dir
        self.cache_expire_days = cache_expire_days
        os.makedirs(self.cache_dir, exist_ok=True)
        
        all_supported = ["ddg", "sogou", "google"]
        # å¤„ç† engine å‚æ•°ä¸ºç©ºçš„æƒ…å†µ
        if not engine or (isinstance(engine, (list, tuple)) and not engine):
            self.engines = all_supported.copy()
        elif isinstance(engine, str):
            if engine.strip() == "":
                self.engines = all_supported.copy()
            else:
                self.engines = [engine.lower()]
        else:
            self.engines = [e.lower() for e in engine]
        self.delay = 1.0  # é»˜è®¤æœç´¢å»¶è¿Ÿ

        # æ£€æŸ¥æ”¯æŒæ€§
        valid_engines = []
        for e in self.engines:
            if e not in all_supported:
                self.logger.warning(f"è­¦å‘Š: ä¸æ”¯æŒçš„æœç´¢å¼•æ“: {e}. è·³è¿‡ã€‚")
                continue
            if e == "sogou" and not SOGOU_AVAILABLE:
                self.logger.warning("è­¦å‘Š: æœç‹—æœç´¢ (k_sogou_search) æœªå®‰è£…, è·³è¿‡æœç‹—ã€‚")
                continue
            valid_engines.append(e)
        if not valid_engines:
            error_msg = "æœªæŒ‡å®šæœ‰æ•ˆçš„æœç´¢å¼•æ“ã€‚"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        self.engines = valid_engines
        self.logger.info(f"ğŸ” æœç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨å¼•æ“: {self.engines}")
        self.logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        self.logger.info(f"â° ç¼“å­˜è¿‡æœŸæ—¶é—´: {self.cache_expire_days} å¤©")
    
    def setup_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•"""
        # åˆ›å»ºlogsç›®å½•
        import os
        os.makedirs("logs", exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"logs/search_engine_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger('SearchEngine')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        self.logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"ğŸ“ æœç´¢å¼•æ“æ—¥å¿—è®°å½•å·²å¯åŠ¨ï¼Œæ—¥å¿—æ–‡ä»¶: {log_filename}")
    
    def _get_cache_key(self, keywords: str, max_results: int, start_date=None, end_date=None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        cache_data = {
            'keywords': keywords,
            'max_results': max_results,
            'start_date': start_date,
            'end_date': end_date,
            'engines': self.engines
        }
        cache_str = json.dumps(cache_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(cache_str.encode('utf-8')).hexdigest()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    def _load_from_cache(self, cache_key: str) -> Dict[str, Any]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_file = self._get_cache_file_path(cache_key)
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            cache_time = datetime.fromisoformat(cache_data['cache_time'])
            expire_time = cache_time + timedelta(days=self.cache_expire_days)
            
            if datetime.now() > expire_time:
                self.logger.info(f"ğŸ“… ç¼“å­˜å·²è¿‡æœŸï¼Œè¿‡æœŸæ—¶é—´: {expire_time}")
                return None
            
            self.logger.info(f"ğŸ“ ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼Œç¼“å­˜æ—¶é—´: {cache_time}")
            return cache_data
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def _save_to_cache(self, cache_key: str, search_results: List[Dict[str, Any]], 
                      keywords: str, max_results: int, start_date=None, end_date=None):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        cache_file = self._get_cache_file_path(cache_key)
        try:
            cache_data = {
                'cache_time': datetime.now().isoformat(),
                'keywords': keywords,
                'max_results': max_results,
                'start_date': start_date,
                'end_date': end_date,
                'engines': self.engines,
                'results': search_results
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ğŸ’¾ æœç´¢ç»“æœå·²ç¼“å­˜åˆ°: {cache_file}")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def search(self, keywords: str, max_results: int = 10, start_date=None, end_date=None, 
               force_refresh: bool = False) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€æœç´¢æ¥å£ï¼Œæ”¯æŒå¤šå¼•æ“åˆå¹¶å»é‡å’Œæœ¬åœ°ç¼“å­˜

        Args:
            keywords: æœç´¢å…³é”®è¯
            max_results: æ¯ä¸ªå¼•æ“æœ€å¤§ç»“æœæ•°
            start_date, end_date: ä»…å¯¹ google æœ‰æ•ˆï¼Œæ ¼å¼ yyyy-mm-dd
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ŒTrueæ—¶å¿½ç•¥ç¼“å­˜ç›´æ¥æœç´¢

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« title, url, description, engine å­—æ®µ
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(keywords, max_results, start_date, end_date)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ·æ–°
        if force_refresh:
            self.logger.info(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå¿½ç•¥ç¼“å­˜ç›´æ¥æœç´¢")
        else:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cache_data = self._load_from_cache(cache_key)
            if cache_data:
                self.logger.info(f"ğŸ“ ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå…³é”®è¯: '{keywords}'")
                return cache_data['results']
            else:
                self.logger.info(f"ğŸ“ ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œè¿›è¡Œç½‘ç»œæœç´¢")
        
        self.logger.info(f"ğŸ” å¼€å§‹æœç´¢ï¼Œä½¿ç”¨å¼•æ“: {','.join([e.upper() for e in self.engines])}")
        self.logger.info(f"ğŸ“ æœç´¢å…³é”®è¯: '{keywords}'")
        self.logger.info(f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}")
        if start_date or end_date:
            self.logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
        
        all_results = {}
        total_results = 0
        
        for engine in self.engines:
            try:
                self.logger.info(f"ğŸš€ å¼€å§‹ {engine.upper()} æœç´¢...")
                start_time = time.time()
                
                if engine == "ddg":
                    results = self._search_ddg(keywords, max_results)
                elif engine == "sogou":
                    results = self._search_sogou(keywords, max_results)
                elif engine == "google":
                    results = self._search_google(keywords, max_results, start_date, end_date)
                else:
                    results = []
                
                search_time = time.time() - start_time
                self.logger.info(f"âœ… {engine.upper()} æœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.2f}ç§’ï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
                
                # å¤„ç†æœç´¢ç»“æœ
                new_results = 0
                duplicate_results = 0
                for r in results:
                    url = r.get('url')
                    if not url:
                        continue
                    if url in all_results:
                        # å·²æœ‰ï¼Œåˆå¹¶ engine å­—æ®µ
                        if engine not in all_results[url]['engine']:
                            all_results[url]['engine'].append(engine)
                        duplicate_results += 1
                    else:
                        # æ–°ç»“æœ
                        r['engine'] = [engine]
                        all_results[url] = r
                        new_results += 1
                
                self.logger.info(f"ğŸ“Š {engine.upper()} ç»“æœç»Ÿè®¡ - æ–°å¢: {new_results}, é‡å¤: {duplicate_results}")
                total_results += len(results)
                
                time.sleep(self.delay)
                
            except Exception as e:
                self.logger.error(f"âŒ {engine.upper()} æœç´¢å¤±è´¥: {e}")
                self.logger.error(f"ğŸ” å¤±è´¥è¯¦æƒ… - å…³é”®è¯: {keywords}, æœ€å¤§ç»“æœæ•°: {max_results}")
        
        # æœ€ç»ˆç»Ÿè®¡
        final_count = len(all_results)
        self.logger.info(f"ğŸ¯ æœç´¢å®Œæˆï¼æ€»è·å¾— {total_results} ä¸ªåŸå§‹ç»“æœï¼Œå»é‡å {final_count} ä¸ªå”¯ä¸€ç»“æœ")
        self.logger.info(f"ğŸ“‹ å»é‡ç‡: {((total_results - final_count) / total_results * 100):.1f}%" if total_results > 0 else "0%")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        final_results = list(all_results.values())
        self._save_to_cache(cache_key, final_results, keywords, max_results, start_date, end_date)
        
        # è¿”å›åˆå¹¶åçš„ç»“æœ
        return final_results

    def _search_ddg(self, keywords: str, max_results: int) -> List[Dict[str, Any]]:
        """DuckDuckGo æœç´¢"""
        self.logger.info(f"ğŸ¦† ä½¿ç”¨ DuckDuckGo æœç´¢: {keywords}")
        try:
            results = DDGS().text(
                keywords=keywords,
                region="cn-zh",
                max_results=max_results
            )
            # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
            formatted_results = [
                {
                    'title': r.get('title', 'æ— æ ‡é¢˜'),
                    'url': r.get('href', 'æ— é“¾æ¥'),
                    'description': r.get('body', 'æ— æ‘˜è¦')
                }
                for r in results
            ]
            self.logger.info(f"âœ… DuckDuckGo æœç´¢æˆåŠŸï¼Œè·å¾— {len(formatted_results)} ä¸ªç»“æœ")
            return formatted_results
        except Exception as e:
            self.logger.error(f"âŒ DuckDuckGo æœç´¢å¼‚å¸¸: {e}")
            return []

    def _search_sogou(self, keywords: str, max_results: int) -> List[Dict[str, Any]]:
        """æœç‹—æœç´¢"""
        self.logger.info(f"ğŸ” ä½¿ç”¨æœç‹—æœç´¢: {keywords}")
        if not SOGOU_AVAILABLE:
            self.logger.warning("âš ï¸ æœç‹—æœç´¢æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡æœç‹—æœç´¢")
            return []
        try:
            # å‡è®¾ sogou_search è¿”å›çš„ç»“æœå·²åŒ…å« title, url, description å­—æ®µ
            results = sogou_search(keywords, num_results=max_results)
            self.logger.info(f"âœ… æœç‹—æœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return results
        except Exception as e:
            self.logger.error(f"âŒ æœç‹—æœç´¢å¼‚å¸¸: {e}")
            return []

    def _search_google(self, keywords: str, max_results: int, start_date=None, end_date=None) -> List[Dict[str, Any]]:
        """Google News æœç´¢"""
        self.logger.info(f"ğŸ“° ä½¿ç”¨ Google News æœç´¢: {keywords}")
        # å¦‚æœæ²¡æœ‰ä¼  start_date/end_dateï¼Œåˆ™ä¸åŠ æ—¶é—´é™åˆ¶
        if not start_date:
            start_date = None
        if not end_date:
            end_date = None
        
        try:
            results = GoogleNewsSearch.search(keywords, max_results, start_date, end_date)
            self.logger.info(f"âœ… Google News æœç´¢æˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªç»“æœ")
            return results
        except Exception as e:
            self.logger.error(f"âŒ Google News æœç´¢å¼‚å¸¸: {e}")
            return []
    
    def clear_cache(self, days_old: int = None):
        """
        æ¸…ç†ç¼“å­˜
        
        Args:
            days_old: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰ç¼“å­˜
        """
        try:
            cleared_count = 0
            current_time = datetime.now()
            
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.cache_dir, filename)
                    file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                    
                    if days_old is None or (current_time - file_time).days > days_old:
                        os.remove(file_path)
                        cleared_count += 1
            
            self.logger.info(f"ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {cleared_count} ä¸ªç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        try:
            cache_files = [f for f in os.listdir(self.cache_dir) if f.endswith('.json')]
            total_size = 0
            cache_details = []
            
            for filename in cache_files:
                file_path = os.path.join(self.cache_dir, filename)
                file_size = os.path.getsize(file_path)
                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
                total_size += file_size
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)
                    keywords = cache_data.get('keywords', 'æœªçŸ¥')
                    results_count = len(cache_data.get('results', []))
                except:
                    keywords = 'è¯»å–å¤±è´¥'
                    results_count = 0
                
                cache_details.append({
                    'filename': filename,
                    'keywords': keywords,
                    'size': file_size,
                    'modified': file_time,
                    'results_count': results_count
                })
            
            return {
                'total_files': len(cache_files),
                'total_size': total_size,
                'cache_dir': self.cache_dir,
                'expire_days': self.cache_expire_days,
                'details': cache_details
            }
        except Exception as e:
            self.logger.error(f"âŒ è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return {}

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯•æœç´¢å¼•æ“å°è£…...")

    # æµ‹è¯•å¤šå¼•æ“åˆå¹¶
    print("\n=== æµ‹è¯•å¤šå¼•æ“åˆå¹¶ ===")
    # engines = ["ddg", "google"]
    # if SOGOU_AVAILABLE:
    #     engines.append("sogou")
    multi_engine = SearchEngine()
    multi_results = multi_engine.search("å•†æ±¤ç§‘æŠ€ è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥", max_results=2, start_date="2024-01-01", end_date="2024-06-01")
    for i, result in enumerate(multi_results, 1):
        print(f"{i}. {result['title']}")
        print(f"   URL: {result['url']}")
        print(f"   æè¿°: {result['description'][:100]}...")
        print(f"   æ¥æºå¼•æ“: {result['engine']}")
        if 'date' in result:
            print(f"   æ—¥æœŸ: {result.get('date', '')}")
        if 'source' in result:
            print(f"   æ¥æº: {result.get('source', '')}")
    
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print("\n=== æµ‹è¯•ç¼“å­˜åŠŸèƒ½ ===")
    cache_info = multi_engine.get_cache_info()
    print(f"ç¼“å­˜ä¿¡æ¯: {cache_info}")
