"""
æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆå™¨
åŒ…å«æ•°æ®é‡‡é›†ã€åˆ†æå’Œæ·±åº¦ç ”æŠ¥ç”Ÿæˆçš„å®Œæ•´æµç¨‹
- ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ
- ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆä¸æ ¼å¼åŒ–è¾“å‡º
"""

import os
import glob
import time
import json
import yaml
import re
import shutil
import requests
import logging
from datetime import datetime
from dotenv import load_dotenv
import importlib
from urllib.parse import urlparse

from data_analysis_agent import quick_analysis
from data_analysis_agent.config.llm_config import LLMConfig
from data_analysis_agent.utils.llm_helper import LLMHelper
from utils.get_shareholder_info import get_shareholder_info, get_table_content
from utils.get_financial_statements import get_all_financial_statements, save_financial_statements_to_csv
from utils.identify_competitors import identify_competitors_with_ai
from utils.get_stock_intro import get_stock_intro, save_stock_intro_to_txt
from duckduckgo_search import DDGS
from utils.markdown_tools import convert_to_docx, format_markdown
from utils.search_engine import SearchEngine

class IntegratedResearchReportGenerator:
    """æ•´åˆçš„ç ”æŠ¥ç”Ÿæˆå™¨ç±»"""
    
    def __init__(self, target_company="å•†æ±¤ç§‘æŠ€", target_company_code="00020", target_company_market="HK", search_engine="all"):
        # é…ç½®æ—¥å¿—è®°å½•
        self.setup_logging()
        
        # ç¯å¢ƒå˜é‡ä¸å…¨å±€é…ç½®
        load_dotenv()
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4")
        # æ‰“å°æ¨¡å‹
        self.logger.info(f"ğŸ”§ ä½¿ç”¨çš„æ¨¡å‹: {self.model}")
        self.target_company = target_company
        self.target_company_code = target_company_code
        self.target_company_market = target_company_market
        
        # æœç´¢å¼•æ“é…ç½®
        self.search_engine = SearchEngine()
        if search_engine and search_engine != "all":
            self.logger.info(f"ğŸ” æœç´¢å¼•æ“å·²é…ç½®ä¸º: {search_engine.upper()}")
        else:
            self.logger.info(f"ğŸ” æœç´¢å¼•æ“é»˜è®¤å…¨éƒ¨ä½¿ç”¨")
        
        # ç›®å½•é…ç½®
        self.data_dir = "./download_financial_statement_files"
        self.company_info_dir = "./company_info"
        self.industry_info_dir = "./industry_info"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        for dir_path in [self.data_dir, self.company_info_dir, self.industry_info_dir]:
            os.makedirs(dir_path, exist_ok=True)
        
        # LLMé…ç½®
        self.llm_config = LLMConfig(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model,
            temperature=0.7,
            max_tokens=8192,
        )
        self.llm = LLMHelper(self.llm_config)
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
    
    def setup_logging(self):
        """é…ç½®æ—¥å¿—è®°å½•"""
        # åˆ›å»ºlogsç›®å½•
        os.makedirs("logs", exist_ok=True)
        
        # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"logs/financial_research_{timestamp}.log"
        
        # é…ç½®æ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger('FinancialResearch')
        self.logger.setLevel(logging.INFO)
        
        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        self.logger.handlers.clear()
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        
        self.logger.info(f"ğŸ“ æ—¥å¿—è®°å½•å·²å¯åŠ¨ï¼Œæ—¥å¿—æ–‡ä»¶: {log_filename}")
    
    def stage1_data_collection(self):
        """ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸš€ å¼€å§‹ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ")
        self.logger.info("="*80)
        
        # 1. è·å–ç«äº‰å¯¹æ‰‹åˆ—è¡¨
        self.logger.info("ğŸ” è¯†åˆ«ç«äº‰å¯¹æ‰‹...")
        other_companies = identify_competitors_with_ai(
            api_key=self.api_key,
            base_url=self.base_url,
            model_name=self.model,
            company_name=self.target_company
        )
        listed_companies = [company for company in other_companies if company.get('market') != "æœªä¸Šå¸‚"]
        
        # 2. è·å–ç›®æ ‡å…¬å¸è´¢åŠ¡æ•°æ®
        self.logger.info(f"\nğŸ“Š è·å–ç›®æ ‡å…¬å¸ {self.target_company} çš„è´¢åŠ¡æ•°æ®...")
        target_financials = get_all_financial_statements(
            stock_code=self.target_company_code,
            market=self.target_company_market,
            period="å¹´åº¦",
            verbose=False
        )
        save_financial_statements_to_csv(
            financial_statements=target_financials,
            stock_code=self.target_company_code,
            market=self.target_company_market,
            company_name=self.target_company,
            period="å¹´åº¦",
            save_dir=self.data_dir
        )
        
        # 3. è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®
        self.logger.info("\nğŸ“Š è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®...")
        competitors_financials = {}
        for company in listed_companies:
            company_name = company.get('name')
            company_code = company.get('code')
            market_str = company.get('market', '')
            
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            
            self.logger.info(f"  è·å– {company_name}({market}:{company_code}) çš„è´¢åŠ¡æ•°æ®")
            try:
                company_financials = get_all_financial_statements(
                    stock_code=company_code,
                    market=market,
                    period="å¹´åº¦",
                    verbose=False
                )
                save_financial_statements_to_csv(
                    financial_statements=company_financials,
                    stock_code=company_code,
                    market=market,
                    company_name=company_name,
                    period="å¹´åº¦",
                    save_dir=self.data_dir
                )
                competitors_financials[company_name] = company_financials
                time.sleep(2)
            except Exception as e:
                self.logger.error(f"  è·å– {company_name} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
        
        # 4. è·å–å…¬å¸åŸºç¡€ä¿¡æ¯
        self.logger.info("\nğŸ¢ è·å–å…¬å¸åŸºç¡€ä¿¡æ¯...")
        all_base_info_targets = [(self.target_company, self.target_company_code, self.target_company_market)]
        
        for company in listed_companies:
            company_name = company.get('name')
            company_code = company.get('code')
            market_str = company.get('market', '')
            if "A" in market_str:
                market = "A"
                if not (company_code.startswith('SH') or company_code.startswith('SZ')):
                    if company_code.startswith('6'):
                        company_code = f"SH{company_code}"
                    else:
                        company_code = f"SZ{company_code}"
            elif "æ¸¯" in market_str:
                market = "HK"
            all_base_info_targets.append((company_name, company_code, market))
        
        # æ·»åŠ ç‰¹å®šå…¬å¸å¦‚ç™¾åº¦
        all_base_info_targets.append(("ç™¾åº¦", "09888", "HK"))
        
        for company_name, company_code, market in all_base_info_targets:
            self.logger.info(f"  è·å– {company_name}({market}:{company_code}) çš„åŸºç¡€ä¿¡æ¯")
            company_info = get_stock_intro(company_code, market=market)
            if company_info:
                save_path = os.path.join(self.company_info_dir, f"{company_name}_{market}_{company_code}_info.txt")
                save_stock_intro_to_txt(company_code, market, save_path)
                self.logger.info(f"    ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")
            else:
                self.logger.warning(f"    æœªèƒ½è·å–åˆ° {company_name} çš„åŸºç¡€ä¿¡æ¯")
            time.sleep(1)
        
        # 5. æœç´¢è¡Œä¸šä¿¡æ¯
        self.logger.info("\nğŸ” æœç´¢è¡Œä¸šä¿¡æ¯...")
        all_search_results = {}
          # æœç´¢ç›®æ ‡å…¬å¸è¡Œä¸šä¿¡æ¯
        target_search_keywords = f"{self.target_company} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ç«äº‰åˆ†æ ä¸šåŠ¡æ¨¡å¼"
        self.logger.info(f"  æ­£åœ¨æœç´¢: {target_search_keywords}")
        # è¿›è¡Œç›®æ ‡å…¬å¸æœç´¢
        target_results = self.search_engine.search(target_search_keywords, 10)
        all_search_results[self.target_company] = target_results

        # æœç´¢ç«äº‰å¯¹æ‰‹è¡Œä¸šä¿¡æ¯
        for company in listed_companies:
            company_name = company.get('name')
            search_keywords = f"{company_name} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥"
            self.logger.info(f"  æ­£åœ¨æœç´¢: {search_keywords}")
            competitor_results = self.search_engine.search(search_keywords, 10)
            all_search_results[company_name] = competitor_results
            # å¢åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            time.sleep(self.search_engine.delay * 2)
        
        # ä¿å­˜æœç´¢ç»“æœ
        search_results_file = os.path.join(self.industry_info_dir, "all_search_results.json")
        with open(search_results_file, 'w', encoding='utf-8') as f:
            json.dump(all_search_results, f, ensure_ascii=False, indent=2)
        
        # 6. è¿è¡Œè´¢åŠ¡åˆ†æ
        self.logger.info("\nğŸ“ˆ è¿è¡Œè´¢åŠ¡åˆ†æ...")
        
        # å•å…¬å¸åˆ†æ
        results = self.analyze_companies_in_directory(self.data_dir, self.llm_config)
        
        # ä¸¤ä¸¤å¯¹æ¯”åˆ†æ
        comparison_results = self.run_comparison_analysis(
            self.data_dir, self.target_company, self.llm_config
        )
        
        # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
        merged_results = self.merge_reports(results, comparison_results)
        
        # å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ
        sensetime_files = self.get_sensetime_files(self.data_dir)
        sensetime_valuation_report = None
        if sensetime_files:
            sensetime_valuation_report = self.analyze_sensetime_valuation(sensetime_files, self.llm_config)
        
        # 7. æ•´ç†æ‰€æœ‰åˆ†æç»“æœ
        self.logger.info("\nğŸ“‹ æ•´ç†åˆ†æç»“æœ...")
        
        # æ•´ç†å…¬å¸ä¿¡æ¯
        company_infos = self.get_company_infos(self.company_info_dir)
        company_infos = self.llm.call(
            f"è¯·æ•´ç†ä»¥ä¸‹å…¬å¸ä¿¡æ¯å†…å®¹ï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°æ˜“è¯»ï¼Œå¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼š\n{company_infos}",
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å¸ä¿¡æ¯æ•´ç†å¸ˆã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        
        # æ•´ç†è‚¡æƒä¿¡æ¯
        info = get_shareholder_info()
        shangtang_shareholder_info = info.get("tables")
        table_content = get_table_content(shangtang_shareholder_info)
        shareholder_analysis = self.llm.call(
            "è¯·åˆ†æä»¥ä¸‹è‚¡ä¸œä¿¡æ¯è¡¨æ ¼å†…å®¹ï¼š\n" + table_content,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ä¸œä¿¡æ¯åˆ†æå¸ˆã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        
        # æ•´ç†è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ
        with open(search_results_file, 'r', encoding='utf-8') as f:
            all_search_results = json.load(f)
        search_res = ""
        for company, results in all_search_results.items():
            search_res += f"ã€{company}æœç´¢ä¿¡æ¯å¼€å§‹ã€‘\n"
            for result in results:
                search_res += f"æ ‡é¢˜: {result.get('title', 'æ— æ ‡é¢˜')}\n"
                search_res += f"é“¾æ¥: {result.get('href', 'æ— é“¾æ¥')}\n"
                search_res += f"æ‘˜è¦: {result.get('body', 'æ— æ‘˜è¦')}\n"
                search_res += "----\n"
            search_res += f"ã€{company}æœç´¢ä¿¡æ¯ç»“æŸã€‘\n\n"
        
        # ä¿å­˜é˜¶æ®µä¸€ç»“æœ
        formatted_report = self.format_final_reports(merged_results)
        
        # ç»Ÿä¸€ä¿å­˜ä¸ºmarkdown
        md_output_file = f"è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å…¬å¸åŸºç¡€ä¿¡æ¯\n\n## æ•´ç†åå…¬å¸ä¿¡æ¯\n\n{company_infos}\n\n")
            f.write(f"# è‚¡æƒä¿¡æ¯åˆ†æ\n\n{shareholder_analysis}\n\n")
            f.write(f"# è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ\n\n{search_res}\n\n")
            f.write(f"# è´¢åŠ¡æ•°æ®åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”\n\n{formatted_report}\n\n")
            if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
                f.write(f"# å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ\n\n{sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š')}\n\n")
        
        self.logger.info(f"\nâœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼åŸºç¡€åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_output_file}")
        
        # å­˜å‚¨ç»“æœä¾›ç¬¬äºŒé˜¶æ®µä½¿ç”¨
        self.analysis_results = {
            'md_file': md_output_file,
            'company_infos': company_infos,
            'shareholder_analysis': shareholder_analysis,
            'search_res': search_res,
            'formatted_report': formatted_report,
            'sensetime_valuation_report': sensetime_valuation_report
        }
        
        return md_output_file
    
    def stage2_deep_report_generation(self, md_file_path):
        """ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ"""
        self.logger.info("\n" + "="*80)
        self.logger.info("ğŸš€ å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ")
        self.logger.info("="*80)
        
        # å¤„ç†å›¾ç‰‡è·¯å¾„
        self.logger.info("ğŸ–¼ï¸ å¤„ç†å›¾ç‰‡è·¯å¾„...")
        new_md_path = md_file_path.replace('.md', '_images.md')
        images_dir = os.path.join(os.path.dirname(md_file_path), 'images')
        self.extract_images_from_markdown(md_file_path, images_dir, new_md_path)
        
        # åŠ è½½æŠ¥å‘Šå†…å®¹
        report_content = self.load_report_content(new_md_path)
        background = self.get_background()
        
        # ç”Ÿæˆå¤§çº²
        self.logger.info("\nğŸ“‹ ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
        parts = self.generate_outline(self.llm, background, report_content)
        
        # åˆ†æ®µç”Ÿæˆæ·±åº¦ç ”æŠ¥
        self.logger.info("\nâœï¸ å¼€å§‹åˆ†æ®µç”Ÿæˆæ·±åº¦ç ”æŠ¥...")
        full_report = ['# å•†æ±¤ç§‘æŠ€å…¬å¸ç ”æŠ¥\n']
        prev_content = ''
        generated_names = set()
        for idx, part in enumerate(parts):
            part_title = part.get('part_title', f'éƒ¨åˆ†{idx+1}')
            if part_title in generated_names:
                self.logger.warning(f"ç« èŠ‚ {part_title} å·²ç”Ÿæˆï¼Œè·³è¿‡")
                self.logger.info(f"åŒæ­¥ç»™LLMï¼šå·²ç”Ÿæˆç« èŠ‚ {list(generated_names)}ï¼Œè·³è¿‡ {part_title}")
                continue
            self.logger.info(f"\n  æ­£åœ¨ç”Ÿæˆï¼š{part_title}")
            is_last = (idx == len(parts) - 1)
            section_text = self.generate_section(
                self.llm, part_title, prev_content, background, report_content, is_last, list(generated_names)
            )
            full_report.append(section_text)
            self.logger.info(f"  âœ… å·²å®Œæˆï¼š{part_title}")
            prev_content = '\n'.join(full_report)
            generated_names.add(part_title)
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report = '\n\n'.join(full_report)
        output_file = f"æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.save_markdown(final_report, output_file)
        
        # æ ¼å¼åŒ–å’Œè½¬æ¢
        self.logger.info("\nğŸ¨ æ ¼å¼åŒ–æŠ¥å‘Š...")
        format_markdown(output_file)
        
        self.logger.info("\nğŸ“„ è½¬æ¢ä¸ºWordæ–‡æ¡£...")
        convert_to_docx(output_file, docx_output=f"{output_file.replace('.md', '.docx')}")
        
        self.logger.info(f"\nâœ… ç¬¬äºŒé˜¶æ®µå®Œæˆï¼æ·±åº¦ç ”æŠ¥å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        self.logger.info("\n" + "="*100)
        self.logger.info("ğŸ¯ å¯åŠ¨æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆæµç¨‹")
        self.logger.info("="*100)
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ
        md_file = self.stage1_data_collection()
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ
        final_report = self.stage2_deep_report_generation(md_file)
        
        self.logger.info("\n" + "="*100)
        self.logger.info("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
        self.logger.info(f"ğŸ“Š åŸºç¡€åˆ†ææŠ¥å‘Š: {md_file}")
        self.logger.info(f"ğŸ“‹ æ·±åº¦ç ”æŠ¥: {final_report}")
        self.logger.info("="*100)
        
        return md_file, final_report

    # ========== è¾…åŠ©æ–¹æ³•ï¼ˆä»åŸå§‹è„šæœ¬ç§»æ¤ï¼‰ ==========
    
    def get_company_infos(self, data_dir="./company_info"):
        """è·å–å…¬å¸ä¿¡æ¯"""
        all_files = os.listdir(data_dir)
        company_infos = ""
        for file in all_files:
            if file.endswith(".txt"):
                company_name = file.split(".")[0]
                with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:
                    content = f.read()
                company_infos += f"ã€å…¬å¸ä¿¡æ¯å¼€å§‹ã€‘\nå…¬å¸åç§°: {company_name}\n{content}\nã€å…¬å¸ä¿¡æ¯ç»“æŸã€‘\n\n"
        return company_infos
    
    def get_company_files(self, data_dir):
        """è·å–å…¬å¸æ–‡ä»¶"""
        all_files = glob.glob(f"{data_dir}/*.csv")
        companies = {}
        for file in all_files:
            filename = os.path.basename(file)
            company_name = filename.split("_")[0]
            companies.setdefault(company_name, []).append(file)
        return companies
    
    def analyze_individual_company(self, company_name, files, llm_config, query=None, verbose=True):
        """åˆ†æå•ä¸ªå…¬å¸"""
        if query is None:
            query = "åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        report = quick_analysis(
            query=query, files=files, llm_config=llm_config, 
            absolute_path=True, max_rounds=20
        )
        return report
    
    def format_final_reports(self, all_reports):
        """æ ¼å¼åŒ–æœ€ç»ˆæŠ¥å‘Š"""
        formatted_output = []
        for company_name, report in all_reports.items():
            formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœå¼€å§‹ã€‘")
            final_report = report.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")
            formatted_output.append(final_report)
            formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœç»“æŸã€‘")
            formatted_output.append("")
        return "\n".join(formatted_output)
    
    def analyze_companies_in_directory(self, data_directory, llm_config, query="åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"):
        """åˆ†æç›®å½•ä¸­çš„æ‰€æœ‰å…¬å¸"""
        company_files = self.get_company_files(data_directory)
        all_reports = {}
        for company_name, files in company_files.items():
            report = self.analyze_individual_company(company_name, files, llm_config, query, verbose=False)
            if report:
                all_reports[company_name] = report
        return all_reports
    
    def compare_two_companies(self, company1_name, company1_files, company2_name, company2_files, llm_config):
        """æ¯”è¾ƒä¸¤ä¸ªå…¬å¸"""
        query = "åŸºäºä¸¤ä¸ªå…¬å¸çš„è¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰å…±åŒç‚¹çš„éƒ¨åˆ†ï¼Œç»˜åˆ¶å¯¹æ¯”åˆ†æçš„è¡¨æ ¼ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        all_files = company1_files + company2_files
        report = quick_analysis(
            query=query,
            files=all_files,
            llm_config=llm_config,
            absolute_path=True,
            max_rounds=20
        )
        return report
    
    def run_comparison_analysis(self, data_directory, target_company_name, llm_config):
        """è¿è¡Œå¯¹æ¯”åˆ†æ"""
        company_files = self.get_company_files(data_directory)
        if not company_files or target_company_name not in company_files:
            return {}
        competitors = [company for company in company_files.keys() if company != target_company_name]
        comparison_reports = {}
        for competitor in competitors:
            comparison_key = f"{target_company_name}_vs_{competitor}"
            report = self.compare_two_companies(
                target_company_name, company_files[target_company_name],
                competitor, company_files[competitor],
                llm_config
            )
            if report:
                comparison_reports[comparison_key] = {
                    'company1': target_company_name,
                    'company2': competitor,
                    'report': report
                }
        return comparison_reports
    
    def merge_reports(self, individual_reports, comparison_reports):
        """åˆå¹¶æŠ¥å‘Š"""
        merged = {}
        for company, report in individual_reports.items():
            merged[company] = report
        for comp_key, comp_data in comparison_reports.items():
            merged[comp_key] = comp_data['report']
        return merged
    
    def get_sensetime_files(self, data_dir):
        """è·å–å•†æ±¤ç§‘æŠ€çš„è´¢åŠ¡æ•°æ®æ–‡ä»¶"""
        all_files = glob.glob(f"{data_dir}/*.csv")
        sensetime_files = []
        for file in all_files:
            filename = os.path.basename(file)
            company_name = filename.split("_")[0]
            if "å•†æ±¤" in company_name or "SenseTime" in company_name:
                sensetime_files.append(file)
        return sensetime_files
    
    def analyze_sensetime_valuation(self, files, llm_config):
        """åˆ†æå•†æ±¤ç§‘æŠ€çš„ä¼°å€¼ä¸é¢„æµ‹"""
        query = "åŸºäºä¸‰å¤§è¡¨çš„æ•°æ®ï¼Œæ„å»ºä¼°å€¼ä¸é¢„æµ‹æ¨¡å‹ï¼Œæ¨¡æ‹Ÿå…³é”®å˜é‡å˜åŒ–å¯¹è´¢åŠ¡ç»“æœçš„å½±å“,å¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
        report = quick_analysis(
            query=query, files=files, llm_config=llm_config, absolute_path=True, max_rounds=20
        )
        return report
    
    # ========== æ·±åº¦ç ”æŠ¥ç”Ÿæˆç›¸å…³æ–¹æ³• ==========
    
    def load_report_content(self, md_path):
        """åŠ è½½æŠ¥å‘Šå†…å®¹"""
        with open(md_path, "r", encoding="utf-8") as f:
            return f.read()
    
    def get_background(self):
        """è·å–èƒŒæ™¯ä¿¡æ¯"""
        return '''
æœ¬æŠ¥å‘ŠåŸºäºè‡ªåŠ¨åŒ–é‡‡é›†ä¸åˆ†ææµç¨‹ï¼Œæ¶µç›–å¦‚ä¸‹ç¯èŠ‚ï¼š
- å…¬å¸åŸºç¡€ä¿¡æ¯ç­‰æ•°æ®å‡é€šè¿‡akshareã€å…¬å¼€å¹´æŠ¥ã€ä¸»æµè´¢ç»æ•°æ®æºè‡ªåŠ¨é‡‡é›†ã€‚
- è´¢åŠ¡ä¸‰å¤§æŠ¥è¡¨æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨-ä¸‰å¤§æŠ¥è¡¨ (https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index)
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç» (https://basic.10jqka.com.cn/new/000066/operate.html)
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ (https://basic.10jqka.com.cn/HK0020/holder.html) é€šè¿‡ç½‘é¡µçˆ¬è™«æŠ€æœ¯è‡ªåŠ¨é‡‡é›†
- è¡Œä¸šä¿¡æ¯é€šè¿‡DuckDuckGoç­‰å…¬å¼€æœç´¢å¼•æ“è‡ªåŠ¨æŠ“å–ï¼Œå¼•ç”¨äº†æƒå¨æ–°é—»ã€ç ”æŠ¥ã€å…¬å¸å…¬å‘Šç­‰ã€‚
- è´¢åŠ¡åˆ†æã€å¯¹æ¯”åˆ†æã€ä¼°å€¼ä¸é¢„æµ‹å‡ç”±å¤§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰è‡ªåŠ¨ç”Ÿæˆï¼Œç»“åˆäº†è¡Œä¸šå¯¹æ ‡ã€è´¢åŠ¡æ¯”ç‡ã€æ²»ç†ç»“æ„ç­‰å¤šç»´åº¦å†…å®¹ã€‚
- ç›¸å…³æ•°æ®ä¸åˆ†æå‡åœ¨è„šæœ¬è‡ªåŠ¨åŒ–æµç¨‹ä¸‹å®Œæˆï¼Œç¡®ä¿æ•°æ®æ¥æºå¯è¿½æº¯ã€åˆ†æé€»è¾‘é€æ˜ã€‚
- è¯¦ç»†å¼•ç”¨ä¸å¤–éƒ¨é“¾æ¥å·²åœ¨æ­£æ–‡ä¸­æ ‡æ³¨ã€‚
- æ•°æ®æ¥å£è¯´æ˜ä¸å…è´£å£°æ˜è§æ–‡æœ«ã€‚
'''
    
    def generate_outline(self, llm, background, report_content):
        """ç”Ÿæˆå¤§çº²"""
        outline_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯å’Œè´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦å°½çš„ã€Šå•†æ±¤ç§‘æŠ€å…¬å¸ç ”æŠ¥ã€‹åˆ†æ®µå¤§çº²ï¼Œè¦æ±‚ï¼š
- ä»¥yamlæ ¼å¼è¾“å‡ºï¼ŒåŠ¡å¿…ç”¨```yamlå’Œ```åŒ…è£¹æ•´ä¸ªyamlå†…å®¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚
- æ¯ä¸€é¡¹ä¸ºä¸€ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†éœ€åŒ…å«ï¼š
  - part_title: ç« èŠ‚æ ‡é¢˜
  - part_desc: æœ¬éƒ¨åˆ†å†…å®¹ç®€ä»‹
- ç« èŠ‚éœ€è¦†ç›–å…¬å¸åŸºæœ¬é¢ã€è´¢åŠ¡åˆ†æã€è¡Œä¸šå¯¹æ¯”ã€ä¼°å€¼ä¸é¢„æµ‹ã€æ²»ç†ç»“æ„ã€æŠ•èµ„å»ºè®®ã€é£é™©æç¤ºã€æ•°æ®æ¥æºç­‰ã€‚
- åªè¾“å‡ºyamlæ ¼å¼çš„åˆ†æ®µå¤§çº²ï¼Œä¸è¦è¾“å‡ºæ­£æ–‡å†…å®¹ã€‚

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
        outline_list = llm.call(
            outline_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ï¼Œå–„äºç»“æ„åŒ–ã€åˆ†æ®µè§„åˆ’è¾“å‡ºï¼Œåˆ†æ®µå¤§çº²å¿…é¡»ç”¨```yamlåŒ…è£¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚",
            max_tokens=4096,
            temperature=0.3
        )
        self.logger.info("\n===== ç”Ÿæˆçš„åˆ†æ®µå¤§çº²å¦‚ä¸‹ =====\n")
        self.logger.info(outline_list)
        try:
            if '```yaml' in outline_list:
                yaml_block = outline_list.split('```yaml')[1].split('```')[0]
            else:
                yaml_block = outline_list
            parts = yaml.safe_load(yaml_block)
            if isinstance(parts, dict):
                parts = list(parts.values())
        except Exception as e:
            self.logger.error(f"[å¤§çº²yamlè§£æå¤±è´¥] {e}")
            parts = []
        return parts
    
    def generate_section(self, llm, part_title, prev_content, background, report_content, is_last, generated_names=None):
        """ç”Ÿæˆç« èŠ‚"""
        if generated_names is None:
            generated_names = []
        section_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç›´æ¥è¾“å‡º\"{part_title}\"è¿™ä¸€éƒ¨åˆ†çš„å®Œæ•´ç ”æŠ¥å†…å®¹ã€‚

ã€å·²ç”Ÿæˆç« èŠ‚ã€‘ï¼š{list(generated_names)}

**é‡è¦è¦æ±‚ï¼š**
1. ç›´æ¥è¾“å‡ºå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ï¼Œä»¥\"## {part_title}\"å¼€å¤´
2. åœ¨æ­£æ–‡ä¸­å¼•ç”¨æ•°æ®ã€äº‹å®ã€å›¾ç‰‡ç­‰ä¿¡æ¯æ—¶ï¼Œé€‚å½“ä½ç½®æ’å…¥å‚è€ƒèµ„æ–™ç¬¦å·ï¼ˆå¦‚[1][2][3]ï¼‰ï¼Œç¬¦å·éœ€ä¸æ–‡æœ«å¼•ç”¨æ–‡çŒ®ç¼–å·ä¸€è‡´
3. **å›¾ç‰‡å¼•ç”¨è¦æ±‚ï¼ˆåŠ¡å¿…ä¸¥æ ¼éµå®ˆï¼‰ï¼š**
   - åªå…è®¸å¼•ç”¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çœŸå®å­˜åœ¨çš„å›¾ç‰‡åœ°å€ï¼ˆæ ¼å¼å¦‚ï¼š./images/å›¾ç‰‡åå­—.pngï¼‰ï¼Œå¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ã€‚
   - ç¦æ­¢è™šæ„ã€æœæ’°ã€æ”¹ç¼–ã€çŒœæµ‹å›¾ç‰‡åœ°å€ï¼Œæœªåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­å‡ºç°çš„å›¾ç‰‡ä¸€å¾‹ä¸å¾—å¼•ç”¨ã€‚
   - å¦‚éœ€æ’å…¥å›¾ç‰‡ï¼Œå¿…é¡»å…ˆåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­æŸ¥æ‰¾ï¼Œæœªæ‰¾åˆ°åˆ™ä¸æ’å…¥å›¾ç‰‡ï¼Œç»ä¸ç¼–é€ å›¾ç‰‡ã€‚
   - å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚
4. ä¸è¦è¾“å‡ºä»»ä½•ã€xxxå¼€å§‹ã€‘ã€xxxç»“æŸã€‘ç­‰åˆ†éš”ç¬¦
5. ä¸è¦è¾“å‡º\"å»ºè®®è¡¥å……\"ã€\"éœ€è¦æ·»åŠ \"ç­‰æç¤ºæ€§è¯­è¨€
6. ä¸è¦ç¼–é€ å›¾ç‰‡åœ°å€æˆ–æ•°æ®
7. å†…å®¹è¦è¯¦å®ã€ä¸“ä¸šï¼Œå¯ç›´æ¥ä½¿ç”¨

**æ•°æ®æ¥æºæ ‡æ³¨ï¼š**
- è´¢åŠ¡æ•°æ®æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨[1]ï¼‰
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç»[2]ï¼‰
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ç½‘é¡µçˆ¬è™«[3]ï¼‰

ã€æœ¬æ¬¡ä»»åŠ¡ã€‘
{part_title}

ã€å·²ç”Ÿæˆå‰æ–‡ã€‘
{prev_content}

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
        if is_last:
            section_prompt += """
è¯·åœ¨æœ¬èŠ‚æœ€åä»¥"å¼•ç”¨æ–‡çŒ®"æ ¼å¼ï¼Œåˆ—å‡ºæ‰€æœ‰æ­£æ–‡ä¸­ç”¨åˆ°çš„å‚è€ƒèµ„æ–™ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[1] ä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨: https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index
[2] åŒèŠ±é¡º-ä¸»è¥ä»‹ç»: https://basic.10jqka.com.cn/new/000066/operate.html
[3] åŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯: https://basic.10jqka.com.cn/HK0020/holder.html
"""
        section_text = llm.call(
            section_prompt,
            system_prompt="ä½ æ˜¯é¡¶çº§é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨ç”Ÿæˆå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ã€‚è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ç ”æŠ¥æ­£æ–‡ï¼Œæ— éœ€ç”¨æˆ·ä¿®æ”¹ã€‚ä¸¥æ ¼ç¦æ­¢è¾“å‡ºåˆ†éš”ç¬¦ã€å»ºè®®æ€§è¯­è¨€æˆ–è™šæ„å†…å®¹ã€‚åªå…è®¸å¼•ç”¨çœŸå®å­˜åœ¨äºã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çš„å›¾ç‰‡åœ°å€ï¼Œä¸¥ç¦è™šæ„ã€çŒœæµ‹ã€æ”¹ç¼–å›¾ç‰‡è·¯å¾„ã€‚å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚",
            max_tokens=8192,
            temperature=0.5
        )
        return section_text
    
    def save_markdown(self, content, output_file):
        """ä¿å­˜markdownæ–‡ä»¶"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        self.logger.info(f"\nğŸ“ æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æå·²ä¿å­˜åˆ°: {output_file}")
     
    # ========== å›¾ç‰‡å¤„ç†ç›¸å…³æ–¹æ³• ==========
    
    def ensure_dir(self, path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        if not os.path.exists(path):
            os.makedirs(path)
    
    def is_url(self, path):
        """åˆ¤æ–­æ˜¯å¦ä¸ºURL"""
        return path.startswith('http://') or path.startswith('https://')
    
    def download_image(self, url, save_path):
        """ä¸‹è½½å›¾ç‰‡"""
        try:
            resp = requests.get(url, stream=True, timeout=10)
            resp.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in resp.iter_content(1024):
                    f.write(chunk)
            return True
        except Exception as e:
            self.logger.error(f"[ä¸‹è½½å¤±è´¥] {url}: {e}")
            return False
    
    def copy_image(self, src, dst):
        """å¤åˆ¶å›¾ç‰‡"""
        try:
            shutil.copy2(src, dst)
            return True
        except Exception as e:
            self.logger.error(f"[å¤åˆ¶å¤±è´¥] {src}: {e}")
            return False
    
    def extract_images_from_markdown(self, md_path, images_dir, new_md_path):
        """ä»markdownä¸­æå–å›¾ç‰‡"""
        self.ensure_dir(images_dir)
        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # åŒ¹é… ![alt](path) å½¢å¼çš„å›¾ç‰‡
        pattern = re.compile(r'!\[[^\]]*\]\(([^)]+)\)')
        matches = pattern.findall(content)
        used_names = set()
        replace_map = {}
        not_exist_set = set()

        for img_path in matches:
            img_path = img_path.strip()
            # å–æ–‡ä»¶å
            if self.is_url(img_path):
                filename = os.path.basename(urlparse(img_path).path)
            else:
                filename = os.path.basename(img_path)
            # é˜²æ­¢é‡å
            base, ext = os.path.splitext(filename)
            i = 1
            new_filename = filename
            while new_filename in used_names:
                new_filename = f"{base}_{i}{ext}"
                i += 1
            used_names.add(new_filename)
            new_img_path = os.path.join(images_dir, new_filename)
            # ä¸‹è½½æˆ–å¤åˆ¶
            img_exists = True
            if self.is_url(img_path):
                success = self.download_image(img_path, new_img_path)
                if not success:
                    img_exists = False
            else:
                # æ”¯æŒç»å¯¹å’Œç›¸å¯¹è·¯å¾„
                abs_img_path = img_path
                if not os.path.isabs(img_path):
                    abs_img_path = os.path.join(os.path.dirname(md_path), img_path)
                if not os.path.exists(abs_img_path):
                    self.logger.warning(f"[è­¦å‘Š] æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨: {abs_img_path}")
                    img_exists = False
                else:
                    self.copy_image(abs_img_path, new_img_path)
            # è®°å½•æ›¿æ¢
            if img_exists:
                replace_map[img_path] = f'./images/{new_filename}'
            else:
                not_exist_set.add(img_path)

        # æ›¿æ¢ markdown å†…å®¹ï¼Œä¸å­˜åœ¨çš„å›¾ç‰‡ç›´æ¥åˆ é™¤æ•´ä¸ªå›¾ç‰‡è¯­æ³•
        def replace_func(match):
            orig = match.group(1).strip()
            if orig in not_exist_set:
                return ''  # åˆ é™¤ä¸å­˜åœ¨çš„å›¾ç‰‡è¯­æ³•
            return match.group(0).replace(orig, replace_map.get(orig, orig))

        new_content = pattern.sub(replace_func, content)
        with open(new_md_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        self.logger.info(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼æ–°æ–‡ä»¶: {new_md_path}")

        # è®°å½•æœªèƒ½æ’å…¥markdownçš„å›¾ç‰‡ä¿¡æ¯
        if not_exist_set:
            for img_path in not_exist_set:
                self.logger.error(f"å›¾ç‰‡æœªèƒ½æ’å…¥markdownï¼ŒåŸå› ï¼šä¸‹è½½/å¤åˆ¶å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨ã€‚åŸå§‹è·¯å¾„: {img_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    parser = argparse.ArgumentParser(description='æ•´åˆçš„é‡‘èç ”æŠ¥ç”Ÿæˆå™¨')
    parser.add_argument('--search-engine', choices=['ddg', 'sogou', 'all'], default='all',
                       help='æœç´¢å¼•æ“é€‰æ‹©: ddg (DuckDuckGo), sogou (æœç‹—), all (å…¨éƒ¨), é»˜è®¤: all')
    parser.add_argument('--company', default='å•†æ±¤ç§‘æŠ€', help='ç›®æ ‡å…¬å¸åç§°')
    parser.add_argument('--code', default='00020', help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--market', default='HK', help='å¸‚åœºä»£ç ')
    parser.add_argument('--stage', choices=['1', '2', 'both'], default='both',
                       help='æ‰§è¡Œé˜¶æ®µ: 1 (ä»…æ•°æ®é‡‡é›†), 2 (ä»…æ·±åº¦ç ”æŠ¥), both (å®Œæ•´æµç¨‹), é»˜è®¤: both')
    parser.add_argument('--input-file', default=None,
                       help='ç¬¬äºŒé˜¶æ®µè¾“å…¥æ–‡ä»¶ (å½“stage=2æ—¶ä½¿ç”¨)')
    parser.add_argument('--output-prefix', default='æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æ',
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--max-search-results', type=int, default=10,
                       help='æ¯æ¬¡æœç´¢çš„æœ€å¤§ç»“æœæ•°')
    parser.add_argument('--force-refresh', action='store_true',
                       help='å¼ºåˆ¶åˆ·æ–°æœç´¢ç¼“å­˜')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = IntegratedResearchReportGenerator(
        target_company=args.company,
        target_company_code=args.code, 
        target_company_market=args.market,
        search_engine=args.search_engine
    )
    
    # æ ¹æ®é˜¶æ®µæ‰§è¡Œä¸åŒçš„æµç¨‹
    if args.stage == '1':
        # ä»…æ‰§è¡Œç¬¬ä¸€é˜¶æ®µ
        logger = logging.getLogger('FinancialResearch')
        logger.info("ğŸš€ ä»…æ‰§è¡Œç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†ä¸åŸºç¡€åˆ†æ")
        basic_report = generator.stage1_data_collection()
        logger.info(f"âœ… ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼åŸºç¡€åˆ†ææŠ¥å‘Š: {basic_report}")
        return basic_report, None
        
    elif args.stage == '2':
        # ä»…æ‰§è¡Œç¬¬äºŒé˜¶æ®µ
        logger = logging.getLogger('FinancialResearch')
        logger.info("ğŸš€ ä»…æ‰§è¡Œç¬¬äºŒé˜¶æ®µï¼šæ·±åº¦ç ”æŠ¥ç”Ÿæˆ")
        
        if args.input_file:
            md_file = args.input_file
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶
            pattern = "è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_*.md"
            files = glob.glob(pattern)
            if not files:
                logger.error("æœªæ‰¾åˆ°è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œç¬¬ä¸€é˜¶æ®µæˆ–æŒ‡å®š --input-file å‚æ•°")
                return None, None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            md_file = files[0]
            logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„è¾“å…¥æ–‡ä»¶: {md_file}")
        
        deep_report = generator.stage2_deep_report_generation(md_file)
        logger.info(f"âœ… ç¬¬äºŒé˜¶æ®µå®Œæˆï¼æ·±åº¦ç ”æŠ¥: {deep_report}")
        return None, deep_report
        
    else:
        # æ‰§è¡Œå®Œæ•´æµç¨‹
        basic_report, deep_report = generator.run_full_pipeline()
        
        # ä½¿ç”¨loggerè®°å½•æœ€ç»ˆç»“æœ
        logger = logging.getLogger('FinancialResearch')
        logger.info("\n" + "="*100)
        logger.info("ğŸ¯ ç¨‹åºæ‰§è¡Œå®Œæ¯•ï¼ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
        logger.info(f"ğŸ“Š åŸºç¡€åˆ†ææŠ¥å‘Š: {basic_report}")
        logger.info(f"ğŸ“‹ æ·±åº¦ç ”æŠ¥: {deep_report}")
        logger.info("="*100)
        
        return basic_report, deep_report


if __name__ == "__main__":
    main()
