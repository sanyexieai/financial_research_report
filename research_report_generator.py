"""
é‡‘èç ”æŠ¥ç”Ÿæˆå™¨
æ•´åˆè´¢åŠ¡åˆ†æã€è‚¡æƒåˆ†æã€ä¼°å€¼æ¨¡å‹å’Œè¡Œä¸šä¿¡æ¯ï¼Œç”Ÿæˆå®Œæ•´çš„é‡‘èç ”æŠ¥
"""

import os
import glob
import time
import json
from datetime import datetime
from dotenv import load_dotenv
import importlib

from data_analysis_agent import quick_analysis
from data_analysis_agent.config.llm_config import LLMConfig
from data_analysis_agent.utils.llm_helper import LLMHelper
from utils.get_shareholder_info import get_shareholder_info, get_table_content
from utils.get_financial_statements import get_all_financial_statements, save_financial_statements_to_csv
from utils.identify_competitors import identify_competitors_with_ai
from utils.get_stock_intro import get_stock_intro, save_stock_intro_to_txt
from duckduckgo_search import DDGS

# ========== ç¯å¢ƒå˜é‡ä¸å…¨å±€é…ç½® ==========
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
model = os.getenv("OPENAI_MODEL", "gpt-4")

target_company = "å•†æ±¤ç§‘æŠ€"
target_company_code = "00020"
target_company_market = "HK"
data_dir = "./download_financial_statement_files"
os.makedirs(data_dir, exist_ok=True)

company_info_dir = "./company_info"
os.makedirs(company_info_dir, exist_ok=True)

llm_config = LLMConfig(
    api_key=api_key,
    base_url=base_url,
    model=model,
    temperature=0.7,
    max_tokens=16384,
)
llm = LLMHelper(llm_config)

# ========== 1. è·å–ç›®æ ‡å…¬å¸åŠç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ® ==========
# è·å–ç«äº‰å¯¹æ‰‹åˆ—è¡¨
other_companies = identify_competitors_with_ai(api_key=api_key,
                                               base_url=base_url,
                                               model_name=model,
                                               company_name=target_company)
listed_companies = [company for company in other_companies if company.get('market') != "æœªä¸Šå¸‚"]

# è·å–ç›®æ ‡å…¬å¸è´¢åŠ¡æ•°æ®
print("\n" + "="*80)
print(f"è·å–ç›®æ ‡å…¬å¸ {target_company}({target_company_market}:{target_company_code}) çš„è´¢åŠ¡æ•°æ®")
target_financials = get_all_financial_statements(
    stock_code=target_company_code,
    market=target_company_market,
    period="å¹´åº¦",
    verbose=False
)
save_financial_statements_to_csv(
    financial_statements=target_financials,
    stock_code=target_company_code,
    market=target_company_market,
    company_name=target_company,
    period="å¹´åº¦",
    save_dir=data_dir
)

# è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®
print("\n" + "="*80)
print("è·å–ç«äº‰å¯¹æ‰‹çš„è´¢åŠ¡æ•°æ®")
competitors_financials = {}
for company in listed_companies:
    company_name = company.get('name')
    company_code = company.get('code')
    market_str = company.get('market', '')
    if "A" in market_str:
        market = "A"
        if not (company_code.startswith('SH') or company_code.startswith('SZ')):
            if company_code.startswith('6'):
                company_code = f"SH{company_code}"
            else:
                company_code = f"SZ{company_code}"
    elif "æ¸¯" in market_str:
        market = "HK"
    print(f"\nè·å–ç«äº‰å¯¹æ‰‹ {company_name}({market}:{company_code}) çš„è´¢åŠ¡æ•°æ®")
    try:
        company_financials = get_all_financial_statements(
            stock_code=company_code,
            market=market,
            period="å¹´åº¦",
            verbose=False
        )
        save_financial_statements_to_csv(
            financial_statements=company_financials,
            stock_code=company_code,
            market=market,
            company_name=company_name,
            period="å¹´åº¦",
            save_dir=data_dir
        )
        competitors_financials[company_name] = company_financials
        time.sleep(2)
    except Exception as e:
        print(f"è·å– {company_name} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
print("\n" + "="*80)
print("è´¢åŠ¡æ•°æ®è·å–å®Œæˆ")
print(f"ç›®æ ‡å…¬å¸: {target_company}")
print(f"ç«äº‰å¯¹æ‰‹æ•°é‡: {len(competitors_financials)}")
print("="*80)

# ========== 1.1 è·å–æ‰€æœ‰å…¬å¸åŸºç¡€ä¿¡æ¯å¹¶ä¿å­˜ ==========
print("="*80)
print("å¼€å§‹è·å–å…¬å¸åŸºç¡€ä¿¡æ¯")
print("="*80)

# ç»Ÿä¸€æ”¶é›†ç›®æ ‡å…¬å¸ã€ç«äº‰å¯¹æ‰‹ã€ç‰¹å®šå…¬å¸ï¼ˆå¦‚ç™¾åº¦ï¼‰
all_base_info_targets = [(target_company, target_company_code, target_company_market)]
for company in listed_companies:
    company_name = company.get('name')
    company_code = company.get('code')
    market_str = company.get('market', '')
    if "A" in market_str:
        market = "A"
        if not (company_code.startswith('SH') or company_code.startswith('SZ')):
            if company_code.startswith('6'):
                company_code = f"SH{company_code}"
            else:
                company_code = f"SZ{company_code}"
    elif "æ¸¯" in market_str:
        market = "HK"
    all_base_info_targets.append((company_name, company_code, market))
# ç‰¹å®šå…¬å¸å¦‚ç™¾åº¦
all_base_info_targets.append(("ç™¾åº¦", "09888", "HK"))

for company_name, company_code, market in all_base_info_targets:
    print(f"\nè·å–å…¬å¸ {company_name}({market}:{company_code}) çš„åŸºç¡€ä¿¡æ¯")
    company_info = get_stock_intro(company_code, market=market)
    if company_info:
        print(company_info)
        save_path = os.path.join(company_info_dir, f"{company_name}_{market}_{company_code}_info.txt")
        save_stock_intro_to_txt(company_code, market, save_path)
        print(f"å…¬å¸ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")
    else:
        print(f"æœªèƒ½è·å–åˆ° {company_name} çš„åŸºç¡€ä¿¡æ¯")
    time.sleep(1)

# ========== 1.x æœç´¢è¡Œä¸šä¿¡æ¯å¹¶ä¿å­˜ ==========
industry_info_dir = "./industry_info"
os.makedirs(industry_info_dir, exist_ok=True)

print("="*80)
print("å¼€å§‹æœç´¢è¡Œä¸šä¿¡æ¯")
print("="*80)

all_search_results = {}

# 1. æœç´¢ç›®æ ‡å…¬å¸è¡Œä¸šä¿¡æ¯
print(f"\næœç´¢ç›®æ ‡å…¬å¸ {target_company} çš„è¡Œä¸šä¿¡æ¯")
target_search_keywords = f"{target_company} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ç«äº‰åˆ†æ ä¸šåŠ¡æ¨¡å¼"
target_results = DDGS().text(
    keywords=target_search_keywords,
    region="cn-zh",
    max_results=10
)
all_search_results[target_company] = target_results

# 2. æœç´¢ç«äº‰å¯¹æ‰‹è¡Œä¸šä¿¡æ¯
print(f"\næœç´¢ç«äº‰å¯¹æ‰‹çš„è¡Œä¸šä¿¡æ¯")
for company in listed_companies:
    company_name = company.get('name')
    search_keywords = f"{company_name} è¡Œä¸šåœ°ä½ å¸‚åœºä»½é¢ ä¸šåŠ¡æ¨¡å¼ å‘å±•æˆ˜ç•¥"
    competitor_results = DDGS().text(
        keywords=search_keywords,
        region="cn-zh",
        max_results=10
    )
    all_search_results[company_name] = competitor_results
    time.sleep(15)

# ä¿å­˜æ‰€æœ‰æœç´¢ç»“æœçš„JSONæ–‡ä»¶
search_results_file = os.path.join(industry_info_dir, "all_search_results.json")
with open(search_results_file, 'w', encoding='utf-8') as f:
    json.dump(all_search_results, f, ensure_ascii=False, indent=2)

# ========== 2. å…¬å¸ä¿¡æ¯æ•´ç† ==========
def get_company_infos(data_dir:str="./company_info"):
    all_files = os.listdir(data_dir)
    company_infos = ""
    for file in all_files:
        if file.endswith(".txt"):
            company_name = file.split(".")[0]
            with open(os.path.join(data_dir, file), 'r', encoding='utf-8') as f:
                content = f.read()
            company_infos += f"ã€å…¬å¸ä¿¡æ¯å¼€å§‹ã€‘\nå…¬å¸åç§°: {company_name}\n{content}\nã€å…¬å¸ä¿¡æ¯ç»“æŸã€‘\n\n"
    return company_infos

company_infos = get_company_infos(company_info_dir)
company_infos = llm.call(
    f"è¯·æ•´ç†ä»¥ä¸‹å…¬å¸ä¿¡æ¯å†…å®¹ï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°æ˜“è¯»ï¼Œå¹¶ä¿ç•™å…³é”®ä¿¡æ¯ï¼š\n{company_infos}",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å¸ä¿¡æ¯æ•´ç†å¸ˆã€‚",
    max_tokens=16384,
    temperature=0.5
)

# ========== 3. è‚¡æƒä¿¡æ¯æ•´ç† ==========
info = get_shareholder_info()
shangtang_shareholder_info = info.get("tables")
table_content = get_table_content(shangtang_shareholder_info)
shareholder_analysis = llm.call(
    "è¯·åˆ†æä»¥ä¸‹è‚¡ä¸œä¿¡æ¯è¡¨æ ¼å†…å®¹ï¼š\n" + table_content,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ä¸œä¿¡æ¯åˆ†æå¸ˆã€‚",
    max_tokens=16384,
    temperature=0.5
)

# ========== 4. è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœæ•´ç† ==========
with open(search_results_file, 'r', encoding='utf-8') as f:
    all_search_results = json.load(f)
search_res = ""
for company, results in all_search_results.items():
    search_res += f"ã€{company}æœç´¢ä¿¡æ¯å¼€å§‹ã€‘\n"
    for result in results:
        search_res += f"æ ‡é¢˜: {result.get('title', 'æ— æ ‡é¢˜')}\n"
        search_res += f"é“¾æ¥: {result.get('href', 'æ— é“¾æ¥')}\n"
        search_res += f"æ‘˜è¦: {result.get('body', 'æ— æ‘˜è¦')}\n"
        search_res += "----\n"
    search_res += f"ã€{company}æœç´¢ä¿¡æ¯ç»“æŸã€‘\n\n"

# ========== 5. è´¢åŠ¡æ•°æ®åˆ†æä¸å¯¹æ¯”åˆ†æ ==========
def get_company_files(data_dir):
    all_files = glob.glob(f"{data_dir}/*.csv")
    companies = {}
    for file in all_files:
        filename = os.path.basename(file)
        company_name = filename.split("_")[0]
        companies.setdefault(company_name, []).append(file)
    return companies

def analyze_individual_company(company_name, files, llm_config, query=None, verbose=True):
    if query is None:
        query = "åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    report = quick_analysis(
        query=query, files=files, llm_config=llm_config, 
        absolute_path=True, max_rounds=20
    )
    return report

def format_final_reports(all_reports):
    formatted_output = []
    for company_name, report in all_reports.items():
        formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœå¼€å§‹ã€‘")
        final_report = report.get("final_report", "æœªç”ŸæˆæŠ¥å‘Š")
        formatted_output.append(final_report)
        formatted_output.append(f"ã€{company_name}è´¢åŠ¡æ•°æ®åˆ†æç»“æœç»“æŸã€‘")
        formatted_output.append("")
    return "\n".join(formatted_output)

def analyze_companies_in_directory(data_directory, llm_config, query="åŸºäºè¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰ä»·å€¼çš„å†…å®¹ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"):
    company_files = get_company_files(data_directory)
    all_reports = {}
    for company_name, files in company_files.items():
        report = analyze_individual_company(company_name, files, llm_config, query, verbose=False)
        if report:
            all_reports[company_name] = report
    return all_reports

def compare_two_companies(company1_name, company1_files, company2_name, company2_files, llm_config):
    query = "åŸºäºä¸¤ä¸ªå…¬å¸çš„è¡¨æ ¼çš„æ•°æ®ï¼Œåˆ†ææœ‰å…±åŒç‚¹çš„éƒ¨åˆ†ï¼Œç»˜åˆ¶å¯¹æ¯”åˆ†æçš„è¡¨æ ¼ï¼Œå¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    all_files = company1_files + company2_files
    report = quick_analysis(
        query=query,
        files=all_files,
        llm_config=llm_config,
        absolute_path=True,
        max_rounds=20
    )
    return report

def run_comparison_analysis(data_directory, target_company_name, llm_config):
    company_files = get_company_files(data_directory)
    if not company_files or target_company_name not in company_files:
        return {}
    competitors = [company for company in company_files.keys() if company != target_company_name]
    comparison_reports = {}
    for competitor in competitors:
        comparison_key = f"{target_company_name}_vs_{competitor}"
        report = compare_two_companies(
            target_company_name, company_files[target_company_name],
            competitor, company_files[competitor],
            llm_config
        )
        if report:
            comparison_reports[comparison_key] = {
                'company1': target_company_name,
                'company2': competitor,
                'report': report
            }
    return comparison_reports

def merge_reports(individual_reports, comparison_reports):
    merged = {}
    for company, report in individual_reports.items():
        merged[company] = report
    for comp_key, comp_data in comparison_reports.items():
        merged[comp_key] = comp_data['report']
    return merged

# ========== 5.1 å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ ==========
def get_sensetime_files(data_dir):
    """è·å–å•†æ±¤ç§‘æŠ€çš„è´¢åŠ¡æ•°æ®æ–‡ä»¶"""
    all_files = glob.glob(f"{data_dir}/*.csv")
    sensetime_files = []
    for file in all_files:
        filename = os.path.basename(file)
        company_name = filename.split("_")[0]
        if "å•†æ±¤" in company_name or "SenseTime" in company_name:
            sensetime_files.append(file)
    return sensetime_files

def analyze_sensetime_valuation(files, llm_config):
    """åˆ†æå•†æ±¤ç§‘æŠ€çš„ä¼°å€¼ä¸é¢„æµ‹"""
    query = "åŸºäºä¸‰å¤§è¡¨çš„æ•°æ®ï¼Œæ„å»ºä¼°å€¼ä¸é¢„æµ‹æ¨¡å‹ï¼Œæ¨¡æ‹Ÿå…³é”®å˜é‡å˜åŒ–å¯¹è´¢åŠ¡ç»“æœçš„å½±å“,å¹¶ç»˜åˆ¶ç›¸å…³å›¾è¡¨ã€‚æœ€åç”Ÿæˆæ±‡æŠ¥ç»™æˆ‘ã€‚"
    report = quick_analysis(
        query=query, files=files, llm_config=llm_config, absolute_path=True, max_rounds=20
    )
    return report

# ========== 6. ä¸»ç¨‹åºå…¥å£ ==========
if __name__ == "__main__":
    # å½“å‰å¯ç”¨çš„ä¸»è¦æ•°æ®è¯´æ˜ï¼š
    print("\n========== æ•°æ®è¯´æ˜ ==========")
    print("1. å…¬å¸åŸºç¡€ä¿¡æ¯ï¼ˆæ•´ç†åï¼‰ï¼šcompany_infos\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(company_infos[:500])  # æ‰“å°å‰500å­—\n")
    print("2. è‚¡æƒä¿¡æ¯åˆ†æï¼ˆæ•´ç†åï¼‰ï¼šshareholder_analysis\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(shareholder_analysis[:500])\n")
    print("3. è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœï¼ˆæ•´ç†åï¼‰ï¼šsearch_res\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(search_res[:500])\n")
    print("4. å•å…¬å¸è´¢åŠ¡åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”åˆ†æç»“æœï¼šmerged_results\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(format_final_reports(merged_results)[:500])\n")
    print("5. å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æï¼šsensetime_valuation_report\n   ç”¨æ³•ç¤ºä¾‹ï¼šprint(sensetime_valuation_report['final_report'][:500])\n")
    print("============================\n")

    # è¿è¡Œå…¬å¸åˆ†æ
    results = analyze_companies_in_directory(
        data_directory=data_dir, 
        llm_config=llm_config
    )
    # è¿è¡Œä¸¤ä¸¤å¯¹æ¯”åˆ†æï¼ˆä»¥å•†æ±¤ç§‘æŠ€ä¸ºç›®æ ‡å…¬å¸ï¼‰
    comparison_results = run_comparison_analysis(
        data_directory=data_dir,
        target_company_name=target_company,
        llm_config=llm_config
    )
    # åˆå¹¶æ‰€æœ‰æŠ¥å‘Š
    merged_results = merge_reports(results, comparison_results)

    # å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ
    sensetime_files = get_sensetime_files(data_dir)
    sensetime_valuation_report = None
    if sensetime_files:
        sensetime_valuation_report = analyze_sensetime_valuation(sensetime_files, llm_config)

    # æ ¼å¼åŒ–å¹¶è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
    if merged_results:
        print("\n" + "="*80)
        print("ğŸ“‹ æ ¼å¼åŒ–è´¢åŠ¡æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆå«ä¸¤ä¸¤å¯¹æ¯”ï¼‰")
        print("="*80)
        formatted_report = format_final_reports(merged_results)
        print(formatted_report)
        output_file = f"è´¢åŠ¡åˆ†ææ±‡æ€»æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(formatted_report)
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        # è¾“å‡ºä¼°å€¼åˆ†ææŠ¥å‘Šä¸»è¦å†…å®¹
        if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
            print("\n" + "="*80)
            print("ğŸ“Š å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†ææŠ¥å‘Šä¸»è¦å†…å®¹ï¼š")
            print("="*80)
            print(sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š'))
        # ç»Ÿä¸€ä¿å­˜ä¸ºmarkdown
        md_output_file = f"è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(md_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å…¬å¸åŸºç¡€ä¿¡æ¯\n\n## æ•´ç†åå…¬å¸ä¿¡æ¯\n\n{company_infos}\n\n")
            f.write(f"# è‚¡æƒä¿¡æ¯åˆ†æ\n\n{shareholder_analysis}\n\n")
            f.write(f"# è¡Œä¸šä¿¡æ¯æœç´¢ç»“æœ\n\n{search_res}\n\n")
            f.write(f"# è´¢åŠ¡æ•°æ®åˆ†æä¸ä¸¤ä¸¤å¯¹æ¯”\n\n{formatted_report}\n\n")
            if sensetime_valuation_report and isinstance(sensetime_valuation_report, dict):
                f.write(f"# å•†æ±¤ç§‘æŠ€ä¼°å€¼ä¸é¢„æµ‹åˆ†æ\n\n{sensetime_valuation_report.get('final_report', 'æœªç”ŸæˆæŠ¥å‘Š')}\n\n")
        print(f"\nğŸ“ Markdownç‰ˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_output_file}")
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸåˆ†æçš„å…¬å¸æ•°æ®")

