"""
æ·±å…¥è´¢åŠ¡ç ”æŠ¥åˆ†æä¸ç”Ÿæˆè„šæœ¬ï¼ˆç®€æ´ç‰ˆï¼‰
åŸºäºè‡ªåŠ¨åŒ–é‡‡é›†ä¸åˆ†æçš„è´¢åŠ¡ç ”æŠ¥æ±‡æ€»ï¼Œç»“åˆå¤§æ¨¡å‹èƒ½åŠ›ï¼Œç”Ÿæˆè¯¦ç»†çš„å…¬å¸è´¢åŠ¡ã€è‚¡æƒã€è¡Œä¸šã€ä¼°å€¼ã€æ²»ç†ç»“æ„ç­‰å¤šç»´åº¦æ·±åº¦åˆ†æä¸æŠ•èµ„å»ºè®®ã€‚
"""

import os
import yaml
from datetime import datetime
from data_analysis_agent.config.llm_config import LLMConfig
from data_analysis_agent.utils.llm_helper import LLMHelper
import re
import shutil
import requests
from urllib.parse import urlparse
import argparse
import logging
import glob

from utils.markdown_tools import convert_to_docx, format_markdown

def load_report_content(md_path):
    with open(md_path, "r", encoding="utf-8") as f:
        return f.read()

def get_background():
    return '''
æœ¬æŠ¥å‘ŠåŸºäºè‡ªåŠ¨åŒ–é‡‡é›†ä¸åˆ†ææµç¨‹ï¼Œæ¶µç›–å¦‚ä¸‹ç¯èŠ‚ï¼š
- å…¬å¸åŸºç¡€ä¿¡æ¯ç­‰æ•°æ®å‡é€šè¿‡akshareã€å…¬å¼€å¹´æŠ¥ã€ä¸»æµè´¢ç»æ•°æ®æºè‡ªåŠ¨é‡‡é›†ã€‚
- è´¢åŠ¡ä¸‰å¤§æŠ¥è¡¨æ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨-ä¸‰å¤§æŠ¥è¡¨ (https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index)
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç» (https://basic.10jqka.com.cn/new/000066/operate.html)
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ (https://basic.10jqka.com.cn/HK0020/holder.html) é€šè¿‡ç½‘é¡µçˆ¬è™«æŠ€æœ¯è‡ªåŠ¨é‡‡é›†
- è¡Œä¸šä¿¡æ¯é€šè¿‡DuckDuckGoç­‰å…¬å¼€æœç´¢å¼•æ“è‡ªåŠ¨æŠ“å–ï¼Œå¼•ç”¨äº†æƒå¨æ–°é—»ã€ç ”æŠ¥ã€å…¬å¸å…¬å‘Šç­‰ã€‚
- è´¢åŠ¡åˆ†æã€å¯¹æ¯”åˆ†æã€ä¼°å€¼ä¸é¢„æµ‹å‡ç”±å¤§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰è‡ªåŠ¨ç”Ÿæˆï¼Œç»“åˆäº†è¡Œä¸šå¯¹æ ‡ã€è´¢åŠ¡æ¯”ç‡ã€æ²»ç†ç»“æ„ç­‰å¤šç»´åº¦å†…å®¹ã€‚
- ç›¸å…³æ•°æ®ä¸åˆ†æå‡åœ¨è„šæœ¬è‡ªåŠ¨åŒ–æµç¨‹ä¸‹å®Œæˆï¼Œç¡®ä¿æ•°æ®æ¥æºå¯è¿½æº¯ã€åˆ†æé€»è¾‘é€æ˜ã€‚
- è¯¦ç»†å¼•ç”¨ä¸å¤–éƒ¨é“¾æ¥å·²åœ¨æ­£æ–‡ä¸­æ ‡æ³¨ã€‚
- æ•°æ®æ¥å£è¯´æ˜ä¸å…è´£å£°æ˜è§æ–‡æœ«ã€‚
'''

def get_llm():
    api_key = os.environ.get("OPENAI_API_KEY")
    base_url = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1")
    model = os.environ.get("OPENAI_MODEL", "gpt-4")
    llm_config = LLMConfig(api_key=api_key, base_url=base_url, model=model)
    return LLMHelper(llm_config)

def generate_outline(llm, background, report_content):
    outline_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹èƒŒæ™¯å’Œè´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ï¼Œç”Ÿæˆä¸€ä»½è¯¦å°½çš„ã€Šå•†æ±¤ç§‘æŠ€å…¬å¸ç ”æŠ¥ã€‹åˆ†æ®µå¤§çº²ï¼Œè¦æ±‚ï¼š
- ä»¥yamlæ ¼å¼è¾“å‡ºï¼ŒåŠ¡å¿…ç”¨```yamlå’Œ```åŒ…è£¹æ•´ä¸ªyamlå†…å®¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚
- æ¯ä¸€é¡¹ä¸ºä¸€ä¸ªä¸»è¦éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†éœ€åŒ…å«ï¼š
  - part_title: ç« èŠ‚æ ‡é¢˜
  - part_desc: æœ¬éƒ¨åˆ†å†…å®¹ç®€ä»‹
- ç« èŠ‚éœ€è¦†ç›–å…¬å¸åŸºæœ¬é¢ã€è´¢åŠ¡åˆ†æã€è¡Œä¸šå¯¹æ¯”ã€ä¼°å€¼ä¸é¢„æµ‹ã€æ²»ç†ç»“æ„ã€æŠ•èµ„å»ºè®®ã€é£é™©æç¤ºã€æ•°æ®æ¥æºç­‰ã€‚
- åªè¾“å‡ºyamlæ ¼å¼çš„åˆ†æ®µå¤§çº²ï¼Œä¸è¦è¾“å‡ºæ­£æ–‡å†…å®¹ã€‚

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
    outline_list = llm.call(
        outline_prompt,
        system_prompt="ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ï¼Œå–„äºç»“æ„åŒ–ã€åˆ†æ®µè§„åˆ’è¾“å‡ºï¼Œåˆ†æ®µå¤§çº²å¿…é¡»ç”¨```yamlåŒ…è£¹ï¼Œä¾¿äºåç»­è‡ªåŠ¨åˆ†å‰²ã€‚",
        max_tokens=4096,
        temperature=0.3
    )
    logger = logging.getLogger('InDepthResearch')
    logger.info("\n===== ç”Ÿæˆçš„åˆ†æ®µå¤§çº²å¦‚ä¸‹ =====\n")
    logger.info(outline_list)
    try:
        if '```yaml' in outline_list:
            yaml_block = outline_list.split('```yaml')[1].split('```')[0]
        else:
            yaml_block = outline_list
        parts = yaml.safe_load(yaml_block)
        if isinstance(parts, dict):
            parts = list(parts.values())
    except Exception as e:
        logger.error(f"[å¤§çº²yamlè§£æå¤±è´¥] {e}")
        parts = []
    return parts

def generate_section(llm, part_title, prev_content, background, report_content, is_last, generated_names=None):
    if generated_names is None:
        generated_names = []
    section_prompt = f"""
ä½ æ˜¯ä¸€ä½é¡¶çº§é‡‘èåˆ†æå¸ˆå’Œç ”æŠ¥æ’°å†™ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç›´æ¥è¾“å‡º\"{part_title}\"è¿™ä¸€éƒ¨åˆ†çš„å®Œæ•´ç ”æŠ¥å†…å®¹ã€‚

ã€å·²ç”Ÿæˆç« èŠ‚ã€‘ï¼š{list(generated_names)}

**é‡è¦è¦æ±‚ï¼š**
1. ç›´æ¥è¾“å‡ºå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ï¼Œä»¥\"## {part_title}\"å¼€å¤´
2. åœ¨æ­£æ–‡ä¸­å¼•ç”¨æ•°æ®ã€äº‹å®ã€å›¾ç‰‡ç­‰ä¿¡æ¯æ—¶ï¼Œé€‚å½“ä½ç½®æ’å…¥å‚è€ƒèµ„æ–™ç¬¦å·ï¼ˆå¦‚[1][2][3]ï¼‰ï¼Œç¬¦å·éœ€ä¸æ–‡æœ«å¼•ç”¨æ–‡çŒ®ç¼–å·ä¸€è‡´
3. **å›¾ç‰‡å¼•ç”¨è¦æ±‚ï¼ˆåŠ¡å¿…ä¸¥æ ¼éµå®ˆï¼‰ï¼š**
   - åªå…è®¸å¼•ç”¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çœŸå®å­˜åœ¨çš„å›¾ç‰‡åœ°å€ï¼ˆæ ¼å¼å¦‚ï¼š./images/å›¾ç‰‡åå­—.pngï¼‰ï¼Œå¿…é¡»ä¸åŸæ–‡å®Œå…¨ä¸€è‡´ã€‚
   - ç¦æ­¢è™šæ„ã€æœæ’°ã€æ”¹ç¼–ã€çŒœæµ‹å›¾ç‰‡åœ°å€ï¼Œæœªåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­å‡ºç°çš„å›¾ç‰‡ä¸€å¾‹ä¸å¾—å¼•ç”¨ã€‚
   - å¦‚éœ€æ’å…¥å›¾ç‰‡ï¼Œå¿…é¡»å…ˆåœ¨ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­æŸ¥æ‰¾ï¼Œæœªæ‰¾åˆ°åˆ™ä¸æ’å…¥å›¾ç‰‡ï¼Œç»ä¸ç¼–é€ å›¾ç‰‡ã€‚
   - å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚
4. ä¸è¦è¾“å‡ºä»»ä½•ã€xxxå¼€å§‹ã€‘ã€xxxç»“æŸã€‘ç­‰åˆ†éš”ç¬¦
5. ä¸è¦è¾“å‡º\"å»ºè®®è¡¥å……\"ã€\"éœ€è¦æ·»åŠ \"ç­‰æç¤ºæ€§è¯­è¨€
6. ä¸è¦ç¼–é€ å›¾ç‰‡åœ°å€æˆ–æ•°æ®
7. å†…å®¹è¦è¯¦å®ã€ä¸“ä¸šï¼Œå¯ç›´æ¥ä½¿ç”¨

**æ•°æ®æ¥æºæ ‡æ³¨ï¼š**
- è´¢åŠ¡æ•°æ®æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨[1]ï¼‰
- ä¸»è¥ä¸šåŠ¡ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-ä¸»è¥ä»‹ç»[2]ï¼‰
- è‚¡ä¸œç»“æ„ä¿¡æ¯æ ‡æ³¨ï¼šï¼ˆæ•°æ®æ¥æºï¼šåŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯ç½‘é¡µçˆ¬è™«[3]ï¼‰

ã€æœ¬æ¬¡ä»»åŠ¡ã€‘
{part_title}

ã€å·²ç”Ÿæˆå‰æ–‡ã€‘
{prev_content}

ã€èƒŒæ™¯è¯´æ˜å¼€å§‹ã€‘
{background}
ã€èƒŒæ™¯è¯´æ˜ç»“æŸã€‘

ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹å¼€å§‹ã€‘
{report_content}
ã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ç»“æŸã€‘
"""
    if is_last:
        section_prompt += """
è¯·åœ¨æœ¬èŠ‚æœ€åä»¥"å¼•ç”¨æ–‡çŒ®"æ ¼å¼ï¼Œåˆ—å‡ºæ‰€æœ‰æ­£æ–‡ä¸­ç”¨åˆ°çš„å‚è€ƒèµ„æ–™ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[1] ä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨: https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index
[2] åŒèŠ±é¡º-ä¸»è¥ä»‹ç»: https://basic.10jqka.com.cn/new/000066/operate.html
[3] åŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯: https://basic.10jqka.com.cn/HK0020/holder.html
"""
    section_text = llm.call(
        section_prompt,
        system_prompt="ä½ æ˜¯é¡¶çº§é‡‘èåˆ†æå¸ˆï¼Œä¸“é—¨ç”Ÿæˆå®Œæ•´å¯ç”¨çš„ç ”æŠ¥å†…å®¹ã€‚è¾“å‡ºå¿…é¡»æ˜¯å®Œæ•´çš„ç ”æŠ¥æ­£æ–‡ï¼Œæ— éœ€ç”¨æˆ·ä¿®æ”¹ã€‚ä¸¥æ ¼ç¦æ­¢è¾“å‡ºåˆ†éš”ç¬¦ã€å»ºè®®æ€§è¯­è¨€æˆ–è™šæ„å†…å®¹ã€‚åªå…è®¸å¼•ç”¨çœŸå®å­˜åœ¨äºã€è´¢åŠ¡ç ”æŠ¥æ±‡æ€»å†…å®¹ã€‘ä¸­çš„å›¾ç‰‡åœ°å€ï¼Œä¸¥ç¦è™šæ„ã€çŒœæµ‹ã€æ”¹ç¼–å›¾ç‰‡è·¯å¾„ã€‚å¦‚å¼•ç”¨äº†ä¸å­˜åœ¨çš„å›¾ç‰‡ï¼Œå°†è¢«åˆ¤ä¸ºé”™è¯¯è¾“å‡ºã€‚",
        max_tokens=8192,
        temperature=0.5
    )
    return section_text

def save_markdown(content, output_file):
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)
    logger = logging.getLogger('InDepthResearch')
    logger.info(f"\nğŸ“ æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æå·²ä¿å­˜åˆ°: {output_file}")

# å›¾ç‰‡è·¯å¾„é¢„å¤„ç†ï¼šå°† md æ–‡ä»¶ä¸­çš„å›¾ç‰‡å…¨éƒ¨æœ¬åœ°åŒ–åˆ° images ç›®å½•ï¼Œå¹¶æ›¿æ¢ä¸º ./images/xxx.png è·¯å¾„
def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def is_url(path):
    return path.startswith('http://') or path.startswith('https://')

def download_image(url, save_path):
    try:
        resp = requests.get(url, stream=True, timeout=10)
        resp.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(1024):
                f.write(chunk)
        return True
    except Exception as e:
        logger = logging.getLogger('InDepthResearch')
        logger.error(f"[ä¸‹è½½å¤±è´¥] {url}: {e}")
        return False

def copy_image(src, dst):
    try:
        shutil.copy2(src, dst)
        return True
    except Exception as e:
        logger = logging.getLogger('InDepthResearch')
        logger.error(f"[å¤åˆ¶å¤±è´¥] {src}: {e}")
        return False

def extract_images_from_markdown(md_path, images_dir, new_md_path):
    ensure_dir(images_dir)
    with open(md_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # åŒ¹é… ![alt](path) å½¢å¼çš„å›¾ç‰‡
    pattern = re.compile(r'!\[[^\]]*\]\(([^)]+)\)')
    matches = pattern.findall(content)
    used_names = set()
    replace_map = {}
    not_exist_set = set()
    logger = logging.getLogger('InDepthResearch')

    for img_path in matches:
        img_path = img_path.strip()
        # å–æ–‡ä»¶å
        if is_url(img_path):
            filename = os.path.basename(urlparse(img_path).path)
        else:
            filename = os.path.basename(img_path)
        # é˜²æ­¢é‡å
        base, ext = os.path.splitext(filename)
        i = 1
        new_filename = filename
        while new_filename in used_names:
            new_filename = f"{base}_{i}{ext}"
            i += 1
        used_names.add(new_filename)
        new_img_path = os.path.join(images_dir, new_filename)
        # ä¸‹è½½æˆ–å¤åˆ¶
        img_exists = True
        if is_url(img_path):
            success = download_image(img_path, new_img_path)
            if not success:
                img_exists = False
        else:
            # æ”¯æŒç»å¯¹å’Œç›¸å¯¹è·¯å¾„
            abs_img_path = img_path
            if not os.path.isabs(img_path):
                abs_img_path = os.path.join(os.path.dirname(md_path), img_path)
            if not os.path.exists(abs_img_path):
                logger.warning(f"[è­¦å‘Š] æœ¬åœ°å›¾ç‰‡ä¸å­˜åœ¨: {abs_img_path}")
                img_exists = False
            else:
                copy_image(abs_img_path, new_img_path)
        # è®°å½•æ›¿æ¢
        if img_exists:
            replace_map[img_path] = f'./images/{new_filename}'
        else:
            not_exist_set.add(img_path)

    # æ›¿æ¢ markdown å†…å®¹ï¼Œä¸å­˜åœ¨çš„å›¾ç‰‡ç›´æ¥åˆ é™¤æ•´ä¸ªå›¾ç‰‡è¯­æ³•
    def replace_func(match):
        orig = match.group(1).strip()
        if orig in not_exist_set:
            return ''  # åˆ é™¤ä¸å­˜åœ¨çš„å›¾ç‰‡è¯­æ³•
        return match.group(0).replace(orig, replace_map.get(orig, orig))

    new_content = pattern.sub(replace_func, content)
    with open(new_md_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    logger.info(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼æ–°æ–‡ä»¶: {new_md_path}")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®æ—¥å¿—è®°å½•
    def setup_logging():
        """é…ç½®æ—¥å¿—è®°å½•"""
        os.makedirs("logs", exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        log_filename = f"logs/in_depth_research_{timestamp}.log"
        
        logger = logging.getLogger('InDepthResearch')
        logger.setLevel(logging.INFO)
        logger.handlers.clear()
        
        file_handler = logging.FileHandler(log_filename, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        logger.info(f"ğŸ“ æ·±åº¦ç ”æŠ¥ç”Ÿæˆæ—¥å¿—å·²å¯åŠ¨: {log_filename}")
        return logger
    
    logger = setup_logging()
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    parser = argparse.ArgumentParser(description='æ·±åº¦è´¢åŠ¡ç ”æŠ¥ç”Ÿæˆå™¨')
    parser.add_argument('--input-file', default=None, 
                       help='è¾“å…¥çš„è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶è·¯å¾„ (é»˜è®¤: è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶)')
    parser.add_argument('--company', default='å•†æ±¤ç§‘æŠ€', help='ç›®æ ‡å…¬å¸åç§°')
    parser.add_argument('--output-prefix', default='æ·±åº¦è´¢åŠ¡ç ”æŠ¥åˆ†æ', 
                       help='è¾“å‡ºæ–‡ä»¶å‰ç¼€')
    parser.add_argument('--format-only', action='store_true', 
                       help='ä»…æ ¼å¼åŒ–ç°æœ‰æ–‡ä»¶ï¼Œä¸é‡æ–°ç”Ÿæˆå†…å®¹')
    
    args = parser.parse_args()
    
    # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
    if args.input_file:
        raw_md_path = args.input_file
        if not os.path.exists(raw_md_path):
            logger.error(f"æŒ‡å®šçš„è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {raw_md_path}")
            return
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶
        pattern = "è´¢åŠ¡ç ”æŠ¥æ±‡æ€»_*.md"
        files = glob.glob(pattern)
        if not files:
            logger.error("æœªæ‰¾åˆ°è´¢åŠ¡ç ”æŠ¥æ±‡æ€»æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é‡‡é›†é˜¶æ®µæˆ–æŒ‡å®š --input-file å‚æ•°")
            return
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
        raw_md_path = files[0]
        logger.info(f"è‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„è¾“å…¥æ–‡ä»¶: {raw_md_path}")
    
    # æ£€æŸ¥æ˜¯å¦ä»…æ ¼å¼åŒ–
    if args.format_only:
        logger.info("ä»…æ ¼å¼åŒ–æ¨¡å¼ï¼Œè·³è¿‡å†…å®¹ç”Ÿæˆ...")
        if os.path.exists(raw_md_path):
            format_markdown(raw_md_path)
            convert_to_docx(raw_md_path,"Company_Research_Report.docx")
        else:
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {raw_md_path}")
        return
    
    # ====== å›¾ç‰‡è·¯å¾„é¢„å¤„ç†ï¼Œè‡ªåŠ¨ç”Ÿæˆæœ¬åœ° images è·¯å¾„çš„ markdown æ–‡ä»¶ ======
    logger.info("ğŸ–¼ï¸ å¼€å§‹å›¾ç‰‡è·¯å¾„é¢„å¤„ç†...")
    new_md_path = raw_md_path.replace('.md', '_images.md')
    images_dir = os.path.join(os.path.dirname(raw_md_path), 'images')
    extract_images_from_markdown(raw_md_path, images_dir, new_md_path)

    # åç»­æµç¨‹ç”¨ new_md_path
    logger.info("ğŸ“– åŠ è½½æŠ¥å‘Šå†…å®¹...")
    report_content = load_report_content(new_md_path)
    background = get_background()
    llm = get_llm()
    
    logger.info("ğŸ“‹ ç”ŸæˆæŠ¥å‘Šå¤§çº²...")
    parts = generate_outline(llm, background, report_content)
    
    logger.info("âœï¸ å¼€å§‹åˆ†æ®µç”Ÿæˆæ·±åº¦ç ”æŠ¥...")
    full_report = [f'# {args.company}å…¬å¸ç ”æŠ¥\n']
    prev_content = ''
    generated_names = set()
    for idx, part in enumerate(parts):
        part_title = part.get('part_title', f'éƒ¨åˆ†{idx+1}')
        if part_title in generated_names:
            logger.warning(f"ç« èŠ‚ {part_title} å·²ç”Ÿæˆï¼Œè·³è¿‡")
            logger.info(f"åŒæ­¥ç»™LLMï¼šå·²ç”Ÿæˆç« èŠ‚ {list(generated_names)}ï¼Œè·³è¿‡ {part_title}")
            continue
        logger.info(f"\n  æ­£åœ¨ç”Ÿæˆï¼š{part_title}")
        is_last = (idx == len(parts) - 1)
        section_text = generate_section(
            llm, part_title, prev_content, background, report_content, is_last, list(generated_names)
        )
        full_report.append(section_text)
        logger.info(f"  âœ… å·²å®Œæˆï¼š{part_title}")
        prev_content = '\n'.join(full_report)
        generated_names.add(part_title)
    
    final_report = '\n\n'.join(full_report)
    output_file = f"{args.output_prefix}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    save_markdown(final_report, output_file)
    
    logger.info("ğŸ¨ æ ¼å¼åŒ–æŠ¥å‘Š...")
    format_markdown(output_file)
    
    logger.info("ğŸ“„ è½¬æ¢ä¸ºWordæ–‡æ¡£...")
    convert_to_docx(output_file)
    
    logger.info(f"\nâœ… æ·±åº¦ç ”æŠ¥ç”Ÿæˆå®Œæˆï¼")
    logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    logger.info(f"ğŸ“„ Wordæ–‡æ¡£: {output_file.replace('.md', '.docx')}")

if __name__ == "__main__":
    main()