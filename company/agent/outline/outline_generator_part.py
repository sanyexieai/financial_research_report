from typing import List, Dict, Any

from company.agent.base_agent import BaseOutlineAgent
from company.model.report_info import ReportInfo

from company.prompt.outline.generate_outline_part import USER_PROMPT_OUTLINE_EDIT_PART, \
    USER_PROMPT_OUTLINE_EDIT_MODIFY_PART, SYSTEM_PROMPT_OUTLINE_PART_EDIT_PART

from llm.schema import Message


class OutlineGeneratorPart(BaseOutlineAgent):
    """å¤§çº²ç”Ÿæˆå™¨"""
    
    def __init__(self, logger, llm):
        super().__init__(logger, llm, SYSTEM_PROMPT_OUTLINE_PART_EDIT_PART)
    
    def generate(self, report_info: ReportInfo, **kwargs) -> List[Dict[str, Any]]:
        """ç”Ÿæˆç ”æŠ¥å¤§çº²"""
        # æ ¹æ®æ˜¯å¦æœ‰æ„è§é€‰æ‹©ä¸åŒçš„æç¤ºè¯
        if report_info.report_outline_opinion:
            self.logger.info("ğŸ“‹ æ­£åœ¨ä¿®æ”¹ç ”æŠ¥å¤§çº²...")
            user_prompt = USER_PROMPT_OUTLINE_EDIT_MODIFY_PART.format(
                target_company=report_info.target_company,
                part_title=report_info.report_title,
                report_data=report_info.rag_company,
                report_outline = report_info.report_outline,
                report_outline_opinion=report_info.report_outline_opinion
            )
        else:
            self.logger.info("ğŸ“‹ æ­£åœ¨ç”Ÿæˆåˆå§‹ç ”æŠ¥å¤§çº²...")
            user_prompt = USER_PROMPT_OUTLINE_EDIT_PART.format(
                target_company=report_info.target_company,
                part_title=report_info.report_title,
                report_data=report_info.rag_company,
            )
        self.logger.info(f"ğŸ“‹ ç”Ÿæˆå¤§çº²çš„æç¤ºè¯ï¼š{user_prompt}")
        report_outline = self._execute_generation(user_prompt)
        report_info.report_outline = report_outline
        return report_outline
    
    def _execute_generation(self, user_prompt: str) -> List[Dict[str, Any]]:
        """æ‰§è¡Œç”Ÿæˆé€»è¾‘"""
        messages = [Message.user_message(user_prompt)]
        self.memory.add_messages(messages)
        
        # Token è®¡ç®—å’Œæ£€æŸ¥
        input_tokens = self._count_tokens(self.messages)
        self.logger.info(f"ğŸ“‹ è¾“å…¥tokens: {input_tokens}")
        
        # è°ƒç”¨ LLM
        response = self.llm.ask(self.memory.messages, temperature=0.3)
        self.logger.info(f"ğŸ“‹ å·²ç”Ÿæˆç ”æŠ¥å¤§çº²ï¼š{response}")
        self._reset_memory()
        return self._parse_yaml_response(response)