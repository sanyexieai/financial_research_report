from company.agent.base_agent import BaseOutlineAgent
from company.prompt.parts.generate_part_edit_part import USER_PROMPT_PART_EDIT_PART, \
    USER_PROMPT_PART_EDIT_MODIFY_PART, SYSTEM_PROMPT_PART_EDIT_PART, USER_PROMPT_PART_EDIT_END_PART
from company.model.report_info import ReportInfo
from llm.schema import Message


class PartGeneratorPart(BaseOutlineAgent):
    """å¤§çº²ç”Ÿæˆå™¨"""
    
    def __init__(self, logger, llm):
        super().__init__(logger, llm, SYSTEM_PROMPT_PART_EDIT_PART)
    
    def generate(self, report_info: ReportInfo, **kwargs) -> str:
        prompt_input = report_info.get_user_prompt_part_input()
        # part = report_info.map_dict_to_cur_part()

        # ä¿®å¤ï¼šæ·»åŠ ç©ºæ ¼
        # æ ¹æ®æ˜¯å¦æœ‰æ„è§é€‰æ‹©ä¸åŒçš„æç¤ºè¯
        cur_subsection_content_opinion = report_info.cur_part_context.cur_subsection_content_opinion
        if cur_subsection_content_opinion:
            self.logger.info("ğŸ“‹ æ­£åœ¨ä¿®æ”¹æ­£æ–‡...")

            user_prompt = USER_PROMPT_PART_EDIT_MODIFY_PART.format(
                **prompt_input
            )
        else:
            if  report_info.cur_part_context.is_report_last:
                self.logger.info("ğŸ“‹ æ­£åœ¨ç”Ÿæˆæœ€åæ­£æ–‡...")
                user_prompt = USER_PROMPT_PART_EDIT_END_PART.format(
                    **prompt_input
                )
                user_prompt += """
                   è¯·åœ¨æœ¬èŠ‚æœ€åä»¥"##  å¼•ç”¨æ–‡çŒ®"æ ¼å¼ï¼Œåˆ—å‡ºæ‰€æœ‰æ­£æ–‡ä¸­ç”¨åˆ°çš„å‚è€ƒèµ„æ–™ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                   [1] ä¸œæ–¹è´¢å¯Œ-æ¸¯è‚¡-è´¢åŠ¡æŠ¥è¡¨: https://emweb.securities.eastmoney.com/PC_HKF10/FinancialAnalysis/index
                   [2] åŒèŠ±é¡º-ä¸»è¥ä»‹ç»: https://basic.10jqka.com.cn/new/000066/operate.html
                   [3] åŒèŠ±é¡º-è‚¡ä¸œä¿¡æ¯: https://basic.10jqka.com.cn/HK0020/holder.html
                   """
            else:
                self.logger.info("ğŸ“‹ æ­£åœ¨ç”Ÿæˆåˆå§‹æ­£æ–‡...")
                user_prompt = USER_PROMPT_PART_EDIT_PART.format(
                    **prompt_input
                )


        self.logger.info(f"ğŸ“‹ ç”Ÿæˆæ­£æ–‡æç¤ºè¯: {user_prompt}")

        generation = self._execute_generation(user_prompt)
        report_info.cur_part_context.cur_subsection_content = generation
        report_info.cur_part_context.cur_content = generation
        return generation
    
    def _execute_generation(self, user_prompt: str) -> str:
        """æ‰§è¡Œç”Ÿæˆé€»è¾‘"""
        messages = [Message.user_message(user_prompt)]
        self.memory.add_messages(messages)
        
        # Token è®¡ç®—å’Œæ£€æŸ¥
        input_tokens = self._count_tokens(self.messages)
        self.logger.info(f"ğŸ“‹ è¾“å…¥tokens: {input_tokens}")
        
        # è°ƒç”¨ LLM
        response = self.llm.ask(self.memory.messages, temperature=0.3)
        self.logger.info(f"ğŸ“‹ å·²ç”Ÿæˆæ­£æ–‡ï¼š{response}")
        self._reset_memory()
        return response