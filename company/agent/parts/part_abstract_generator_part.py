from company.agent.base_agent import BaseOutlineAgent
from company.prompt.parts.generate_part_abstract_part import SYSTEM_PROMPT_PART_ABSTRACT_PART, \
    USER_PROMPT_PART_ABSTRACT_PART
from company.model.report_info import ReportInfo
from llm.schema import Message


class PartAbstractGeneratorPart(BaseOutlineAgent):
    """å¤§çº²ç”Ÿæˆå™¨"""
    
    def __init__(self, logger, llm):
        super().__init__(logger, llm, SYSTEM_PROMPT_PART_ABSTRACT_PART)
    
    def generate(self, report_info: ReportInfo, **kwargs) -> str:

        report_text_list = report_info.report_text_list
        if not report_text_list:
            self.logger.info("ğŸ“‹ æ²¡æœ‰æ–‡ç« ï¼Œè·³è¿‡")
            return ""

        self.logger.info("ğŸ“‹ æ­£åœ¨ç”Ÿæˆæ­£æ–‡ç« èŠ‚æ‘˜è¦...")
        user_prompt = USER_PROMPT_PART_ABSTRACT_PART.format(
            report_text_list = report_text_list,
        )
        self.logger.info(f"ğŸ“‹ ç”Ÿæˆæ­£æ–‡ç« èŠ‚æ‘˜è¦æç¤ºè¯: {user_prompt}")

        generation = self._execute_generation(user_prompt)
        report_info.cur_part_context.prev_part_content_abstract = generation
        return generation
    
    def _execute_generation(self, user_prompt: str) -> str:
        """æ‰§è¡Œç”Ÿæˆé€»è¾‘"""
        messages = [Message.user_message(user_prompt)]
        self.memory.add_messages(messages)
        
        # Token è®¡ç®—å’Œæ£€æŸ¥
        input_tokens = self._count_tokens(self.messages)
        self.logger.info(f"ğŸ“‹ è¾“å…¥tokens: {input_tokens}")
        
        # è°ƒç”¨ LLM
        response = self.llm.ask(self.memory.messages, temperature=0.3)
        self.logger.info(f"ğŸ“‹ å·²æ­£æ–‡ç« èŠ‚æ‘˜è¦ï¼š{response}")
        self._reset_memory()
        return response